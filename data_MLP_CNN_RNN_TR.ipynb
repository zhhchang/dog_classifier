{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"colab":{"name":"data_MLP_CNN_RNN_TR.ipynb","provenance":[],"collapsed_sections":["ce972947","2a9a687b","2b73143a"]}},"cells":[{"cell_type":"code","metadata":{"id":"7e4df1d7","executionInfo":{"status":"ok","timestamp":1637239447105,"user_tz":-480,"elapsed":1875,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from random import randint\n","import time\n","import os\n","import utils\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from PIL import Image\n","from torchvision.transforms import transforms\n","import requests\n","from os.path import dirname, exists\n","from os import makedirs\n","import pandas as pd\n","import math\n","import sys\n","from torchvision import datasets, models, transforms\n","from torch.optim import lr_scheduler\n","import copy\n","from torch.utils.data import DataLoader\n"],"id":"7e4df1d7","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":375},"id":"b86828f1","executionInfo":{"status":"error","timestamp":1637238237617,"user_tz":-480,"elapsed":380,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}},"outputId":"5606eb70-95f8-4e85-b691-804c66648f50"},"source":["# dataset image path\n","train_img_path = '../dog_dataset/train_64x64'\n","test_img_path = '../dog_dataset/test_64x64'\n","\n","# rgb dataset .pt path\n","train_data=torch.load('../dog_dataset/train_data_final.pt')\n","train_label=torch.load('../dog_dataset/train_label_final.pt')\n","test_data=torch.load('../dog_dataset/test_data_final.pt')\n","test_label=torch.load('../dog_dataset/test_label_final.pt')"],"id":"b86828f1","execution_count":3,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-7b291dbe4a42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# rgb dataset .pt path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../dog_dataset/train_data_final.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../dog_dataset/train_label_final.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../dog_dataset/test_data_final.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../dog_dataset/train_data_final.pt'"]}]},{"cell_type":"markdown","metadata":{"id":"b556d4df"},"source":["# Dataset Collection #"],"id":"b556d4df"},{"cell_type":"markdown","metadata":{"id":"2b2f267c"},"source":["Generate scraper to download the images from https://dog.ceo/dog-api/ for all the breeds to form our raw dataset."],"id":"2b2f267c"},{"cell_type":"code","metadata":{"id":"74b7d822","executionInfo":{"status":"aborted","timestamp":1637237419224,"user_tz":-480,"elapsed":21,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["project_path = dirname(__file__)\n","data_dir = project_path + \"/data\"\n","if not exists(data_dir):\n","    makedirs(data_dir)\n","\n","\n","breed_url = \"https://dog.ceo/api/breeds/list/all\"\n","breed_response = requests.get(breed_url)\n","breed_dict = breed_response.json().get('message')\n","single_breed = [x for x in breed_dict if len(breed_dict.get(x)) == 0]\n","multi_breed = [i+\"/\"+j for i in breed_dict for j in breed_dict.get(i) if len(breed_dict.get(i)) > 0]\n","breed_lst = single_breed + multi_breed\n","\n","\n","def get_img_url(breed):\n","    url = f\"https://dog.ceo/api/breed/{breed}/images\"\n","    img_response = requests.get(url)\n","    img_lst = img_response.json().get('message')\n","    if not exists(f\"{data_dir}/{breed}\"):\n","        makedirs(f\"{data_dir}/{breed}\")\n","    for i in img_lst:\n","        download_img(breed, i)\n","\n","\n","def download_img(breed, url):\n","    response = requests.get(url)\n","    with open(f\"{data_dir}/{breed}/{url.split('/')[-1]}\", \"wb\") as file:\n","        file.write(response.content)\n","\n","\n","for i in breed_lst:\n","    get_img_url(i)\n"],"id":"74b7d822","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8e911256"},"source":["# Exploratory Data Analysis And Data Preprocessing #"],"id":"8e911256"},{"cell_type":"markdown","metadata":{"id":"d9734129"},"source":["## First Look of the Raw Dataset ##"],"id":"d9734129"},{"cell_type":"markdown","metadata":{"id":"8923a0a0"},"source":["A glance of the raw dataset downloaded from Dog API using the scraper \n","<div>\n","    <img src=\"report_attachment/raw_data_look.png\" width='500' align='left'>\n","</div>"],"id":"8923a0a0"},{"cell_type":"code","metadata":{"id":"a2fda93e","executionInfo":{"status":"aborted","timestamp":1637237419225,"user_tz":-480,"elapsed":22,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["raw_data_path = '../data'\n","print('Total number of breeds in the raw dataset: ', len([i for i in os.listdir(raw_data_path)]))"],"id":"a2fda93e","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2c2a1fc3"},"source":["## Dataset Preperation ##"],"id":"2c2a1fc3"},{"cell_type":"markdown","metadata":{"id":"3e886b9f"},"source":["There are in total 91 dog breeds in the raw dataset.  \n","However, the data sizes for each breed are varing from a dozen to more than a hundred.  \n","To create a balanced dataset, we have filtered **10 most popular dog breeds** out of the list and with data size **more than 100 images** .  \n","Below is the list of dog breeds to be classified in our classsifier."],"id":"3e886b9f"},{"cell_type":"code","metadata":{"id":"6295c485","executionInfo":{"status":"aborted","timestamp":1637237419225,"user_tz":-480,"elapsed":21,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["# list of dog breeds to be classified in our dog classifier\n","dog_dict = {\n","    'affenpinscher':0,\n","    'beagle':1,\n","    'boxer':2,\n","    'chihuahua':3,\n","    'frenchbulldog':4,\n","    'goldenretriever':5,\n","    'rottweiler':6,\n","    'schnauzer':7,\n","    'sheepdog':8,\n","    'spaniel':9\n","}"],"id":"6295c485","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a3a6afa5"},"source":["To improve the quality of the dataset, we screen through the images and unsuitable images are removed from the dataset, e.g., images with vague objects.  \n","After data cleaning, the dataset is seperated into 2 sets: train set and test set with balanced class distribution."],"id":"a3a6afa5"},{"cell_type":"code","metadata":{"id":"9171e847","executionInfo":{"status":"aborted","timestamp":1637237419226,"user_tz":-480,"elapsed":22,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["print('train set size: ', train_data.size()[0])\n","print('test set size: ', test_data.size()[0])"],"id":"9171e847","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2d2e8469","executionInfo":{"status":"aborted","timestamp":1637237419227,"user_tz":-480,"elapsed":23,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["fig = plt.figure()\n","\n","# train data class distribution\n","ax1 = fig.add_axes([0, 0, 1, 1], aspect=1)\n","plt.title(\"train data\")\n","data = train_label.numpy()\n","data = pd.Series(data).value_counts()\n","label_num = data.index.tolist()\n","label = []\n","for i in range(len(label_num)):\n","    label.append([k for k, v in dog_dict.items() if v == label_num[i]])\n","label = np.asarray(label).squeeze()\n","ax1.pie(data, labels = label, autopct='%.2f%%')\n","\n","# test data class distribution\n","ax2 = fig.add_axes([1, .0, 1, 1], aspect=1)\n","plt.title(\"test data\")\n","data = test_label.numpy()\n","data = pd.Series(data).value_counts()\n","label_num = data.index.tolist()\n","label = []\n","for i in range(len(label_num)):\n","    label.append([k for k, v in dog_dict.items() if v == label_num[i]])\n","label = np.asarray(label).squeeze()\n","ax2.pie(data, labels = label, autopct='%.2f%%')\n","plt.show()"],"id":"2d2e8469","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ff85ffcd"},"source":["## Data Preprocessing ##"],"id":"ff85ffcd"},{"cell_type":"markdown","metadata":{"id":"d885dd61"},"source":[" ### Crop Images ###"],"id":"d885dd61"},{"cell_type":"markdown","metadata":{"id":"7b18019e"},"source":["Manually crop dataset images to 1:1 with clear and complete object"],"id":"7b18019e"},{"cell_type":"markdown","metadata":{"id":"b696fdc1"},"source":["### Create Different Dataset with Corresponding Label ###"],"id":"b696fdc1"},{"cell_type":"markdown","metadata":{"id":"ce972947"},"source":["#### 64x64 RGB Dataset ####"],"id":"ce972947"},{"cell_type":"code","metadata":{"id":"fc253088","executionInfo":{"status":"aborted","timestamp":1637237419228,"user_tz":-480,"elapsed":24,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["# resize rgb data to 64x64\n","resize_norm = img_trans = transforms.Compose([transforms.Resize([64,64]),\n","                                              transforms.ToTensor()])\n","    \n","# train data\n","train_data = torch.Tensor([])\n","train_label = []\n","for img in os.listdir(train_img_path):\n","    pic_data = []\n","    breed = img.split('_')[0]\n","    pic_label = dog_dict[breed]\n","    train_label.append(pic_label)\n","    pic = Image.open(os.path.join(train_img_path,img)) \n","    trans_pic = resize_norm(pic)\n","    trans_pic = torch.unsqueeze(trans_pic, dim=0)\n","    train_data = torch.cat((train_data, trans_pic),0)\n","    \n","# test data\n","test_data = torch.Tensor([])\n","test_label = []\n","for img in os.listdir(test_img_path):\n","    pic_data = []\n","    breed = img.split('_')[0]\n","    pic_label = dog_dict[breed]\n","    test_label.append(pic_label)\n","    pic = Image.open(os.path.join(test_img_path,img)) \n","    trans_pic = resize_norm(pic)\n","    trans_pic = torch.unsqueeze(trans_pic, dim=0)\n","    test_data = torch.cat((test_data, trans_pic),0)\n","\n","# change label from list to tensor\n","train_label = torch.tensor(train_label)\n","test_label = torch.tensor(test_label)\n","\n","# save train data and label to .pt file\n","torch.save(train_data, '../dog_dataset/train_data_final.pt')\n","torch.save(train_label, '../dog_dataset/train_label_final.pt')\n","torch.save(test_data, '../dog_dataset/test_data_final.pt')\n","torch.save(test_label, '../dog_dataset/test_label_final.pt')"],"id":"fc253088","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dff874d8","executionInfo":{"status":"aborted","timestamp":1637237419228,"user_tz":-480,"elapsed":23,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["utils.show(train_data[4])"],"id":"dff874d8","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2a9a687b"},"source":["#### 64x64 Grayscale Dataset ####"],"id":"2a9a687b"},{"cell_type":"code","metadata":{"id":"b34de63a","executionInfo":{"status":"aborted","timestamp":1637237419229,"user_tz":-480,"elapsed":24,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["# rgb to gray\n","rgb_to_gray = transforms.Compose([transforms.Resize([64,64]),\n","                                  transforms.Grayscale(num_output_channels=1),\n","                                  transforms.ToTensor()])\n","\n","# train data\n","train_data_gray = torch.Tensor([])\n","train_label_gray = []\n","for img in os.listdir(train_img_path):\n","    pic_data = []\n","    breed = img.split('_')[0]\n","    pic_label = dog_dict[breed]\n","    train_label_gray.append(pic_label)\n","    pic = Image.open(os.path.join(train_img_path,img))\n","    pic = rgb_to_gray(pic)\n","    train_data_gray = torch.cat((train_data_gray, pic),0)\n","    \n","# test data\n","test_data_gray = torch.Tensor([])\n","test_label_gray = []\n","for img in os.listdir(test_img_path):\n","    pic_data = []\n","    breed = img.split('_')[0]\n","    pic_label = dog_dict[breed]\n","    test_label_gray.append(pic_label)\n","    pic = Image.open(os.path.join(test_img_path,img))\n","    pic = rgb_to_gray(pic)\n","    test_data_gray = torch.cat((test_data_gray, pic),0)\n","    \n","# change label from list to tensor\n","train_label_gray = torch.tensor(train_label_gray)\n","test_label_gray = torch.tensor(test_label_gray)\n","    \n","# save train data and label to .pt file\n","torch.save(train_data_gray, '../dog_dataset/train_data_gray.pt')\n","torch.save(train_label_gray, '../dog_dataset/train_label_gray.pt')\n","torch.save(test_data_gray, '../dog_dataset/test_data_gray.pt')\n","torch.save(test_label_gray, '../dog_dataset/test_label_gray.pt')"],"id":"b34de63a","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9bce1ba5","executionInfo":{"status":"aborted","timestamp":1637237419229,"user_tz":-480,"elapsed":24,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["utils.show(train_data_gray[4])"],"id":"9bce1ba5","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2b73143a"},"source":["#### 64x64 Image Augmentation ####"],"id":"2b73143a"},{"cell_type":"code","metadata":{"id":"1cc5cbc9","executionInfo":{"status":"aborted","timestamp":1637237419229,"user_tz":-480,"elapsed":24,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["# image augmentation\n","img_augmentation = transforms.Compose([transforms.Resize([64,64]),\n","                                        transforms.RandomHorizontalFlip(),\n","                                        transforms.ColorJitter(brightness=0.3, contrast=0.3),\n","                                        transforms.ToTensor()])\n","\n","#train data\n","train_data_aug = torch.Tensor([])\n","train_label_aug = []\n","for img in os.listdir(train_img_path):\n","    pic_data = []\n","    breed = img.split('_')[0]\n","    pic_label = dog_dict[breed]\n","    train_label_aug.append(pic_label)\n","    pic = Image.open(os.path.join(train_img_path,img)) \n","    trans_pic = img_augmentation(pic)\n","    trans_pic = torch.unsqueeze(trans_pic, dim=0)\n","    train_data_aug = torch.cat((train_data_aug, trans_pic),0)\n","    \n","# change label from list to tensor\n","train_label_aug = torch.tensor(train_label_aug)\n","\n","# save test data and label to .pt file\n","torch.save(train_data_aug, '../dog_dataset/train_data_aug.pt')\n","torch.save(train_label_aug, '../dog_dataset/train_label_aug.pt')"],"id":"1cc5cbc9","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ded820b8"},"source":["Create augmented dataset by random flip, brightness and contrast adjustment.  \n","(top: original data, bottom: augmented data)  \n","<div>\n","    <img src=\"report_attachment/image_aug_compare.png\" width='250' align='left'>\n","</div>"],"id":"ded820b8"},{"cell_type":"markdown","metadata":{"id":"e60c73f7"},"source":["## MLP"],"id":"e60c73f7"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0d98bf66","executionInfo":{"status":"ok","timestamp":1637239452516,"user_tz":-480,"elapsed":8,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}},"outputId":"3af26ba4-e239-41a3-c4fe-444c95433a35"},"source":["device= torch.device(\"cuda\")\n","print(device)"],"id":"0d98bf66","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"markdown","metadata":{"id":"f4d6c634"},"source":["### Load RGD Dataset"],"id":"f4d6c634"},{"cell_type":"code","metadata":{"id":"5bde7f94","executionInfo":{"status":"aborted","timestamp":1637237419230,"user_tz":-480,"elapsed":24,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["dataset_folder=''\n","train_data=torch.load(dataset_folder+'train_data_final.pt')\n","print(train_data.size())\n","\n","test_data=torch.load(dataset_folder+'test_data_final.pt')\n","print(test_data.size())\n","\n","train_label=torch.load(dataset_folder+'train_label_final.pt')\n","print(train_label.size())\n","\n","test_label=torch.load(dataset_folder+'test_label_final.pt')\n","print(test_label.size())"],"id":"5bde7f94","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a51d839f","executionInfo":{"status":"aborted","timestamp":1637237419231,"user_tz":-480,"elapsed":25,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["class three_layer_net(nn.Module):\n","\n","    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3,output_size):\n","        super(three_layer_net , self).__init__()\n","\n","        self.layer1 = nn.Linear(  input_size   , hidden_size1  , bias=False  )\n","        self.layer2 = nn.Linear(  hidden_size1 , hidden_size2  , bias=False  )\n","        self.layer3 = nn.Linear(  hidden_size2 , hidden_size3  , bias=False  )\n","        self.layer4 = nn.Linear(  hidden_size3 , output_size   , bias=False  )        \n","        \n","    def forward(self, x):\n","        \n","        y       = self.layer1(x)\n","        y_hat   = torch.relu(y)\n","        z       = self.layer2(y_hat)\n","        z_hat   = torch.relu(z)\n","        a       = self.layer3(z_hat)\n","        a_hat   = torch.relu(a)\n","        scores  = self.layer4(a_hat)\n","        \n","        return scores"],"id":"a51d839f","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"570b9891","executionInfo":{"status":"aborted","timestamp":1637237419231,"user_tz":-480,"elapsed":25,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["net=three_layer_net(12288,500,500,500,11)\n","print(net)\n","criterion = nn.CrossEntropyLoss()\n","optimizer=torch.optim.SGD( net.parameters() , lr=0.01 )\n","bs= 10"],"id":"570b9891","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"236d2782","executionInfo":{"status":"aborted","timestamp":1637237419232,"user_tz":-480,"elapsed":25,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["def eval_on_test_set_mlp():\n","\n","    running_error=0\n","    num_batches=0\n","\n","\n","    for i in range(0,100,bs):\n","\n","        # extract the minibatch\n","        minibatch_data =  test_data[i:i+bs]\n","        minibatch_label= test_label[i:i+bs]\n","        # send them to the gpu\n","        #minibatch_data=minibatch_data.to(device)\n","        #minibatch_label=minibatch_label.to(device)\n","\n","        # reshape the minibatch\n","        inputs = minibatch_data.view(bs,12288)\n","\n","        # feed it to the network\n","        scores=net( inputs ) \n","\n","        # compute the error made on this batch\n","        error = utils.get_error( scores , minibatch_label)\n","\n","        # add it to the running error\n","        running_error += error.item()\n","\n","        num_batches+=1\n","\n","\n","    # compute error rate on the full test set\n","    total_error = running_error/num_batches\n","\n","    print( 'error rate on test set =', total_error*100 ,'percent')"],"id":"236d2782","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d8e8eb7c","executionInfo":{"status":"aborted","timestamp":1637237419232,"user_tz":-480,"elapsed":25,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["start=time.time()\n","\n","for epoch in range(10):\n","    \n","    running_loss=0\n","    running_error=0\n","    num_batches=0\n","    \n","    shuffled_indices=torch.randperm(970)\n"," \n","    for count in range(0,970,bs):\n","    \n","        # Set the gradients to zeros\n","        optimizer.zero_grad()\n","        \n","        # create a minibatch       \n","        indices=shuffled_indices[count:count+bs]\n","        minibatch_data =  train_data[indices]\n","        minibatch_label=  train_label[indices]\n","        \n","        # send them to the gpu\n","        #minibatch_data=minibatch_data.to(device)\n","        #minibatch_label=minibatch_label.to(device)\n","        \n","        # reshape the minibatch\n","        inputs = minibatch_data.view(bs,12288)\n","\n","        # tell Pytorch to start tracking all operations that will be done on \"inputs\"\n","        inputs.requires_grad_()\n","\n","        # forward the minibatch through the net \n","        scores=net( inputs ) \n","\n","        # Compute the average of the losses of the data points in the minibatch\n","        loss =  criterion( scores , minibatch_label) \n","        \n","        # backward pass to compute dL/dU, dL/dV and dL/dW   \n","        loss.backward()\n","\n","        # do one step of stochastic gradient descent: U=U-lr(dL/dU), V=V-lr(dL/dU), ...\n","        optimizer.step()\n","        \n","\n","        # START COMPUTING STATS\n","        \n","        # add the loss of this batch to the running loss\n","        running_loss += loss.detach().item()\n","        \n","        # compute the error made on this batch and add it to the running error       \n","        error = utils.get_error( scores.detach() , minibatch_label)\n","        running_error += error.item()\n","        \n","        num_batches+=1        \n","    \n","    \n","    # compute stats for the full training set\n","    total_loss = running_loss/num_batches\n","    total_error = running_error/num_batches\n","    elapsed = time.time()-start\n","    \n","#if epoch%2 == 0:\n","    print('epoch=',epoch, '\\t time=', elapsed, '\\t loss=', total_loss , '\\t error=', total_error*100 ,'percent')\n","    eval_on_test_set_mlp() \n","    print(' ')"],"id":"d8e8eb7c","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"83b2a915"},"source":["## CNN for Dog Breed Classification"],"id":"83b2a915"},{"cell_type":"code","metadata":{"id":"e7983848","executionInfo":{"status":"aborted","timestamp":1637237419233,"user_tz":-480,"elapsed":26,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["class Vgg_convnet(nn.Module):\n","\n","    def __init__(self):\n","\n","        super(Vgg_convnet, self).__init__()\n","\n","        # block 1:         3 x 64 x 64 --> 64 x 32 x 32        \n","        self.conv1a = nn.Conv2d(3,   64,  kernel_size=8, padding=4 )\n","        # self.conv1b = nn.Conv2d(64,  64,  kernel_size=3, padding=1 )\n","        self.pool1  = nn.MaxPool2d(2,2)\n","\n","        # block 2:         64 x 32 x 32 --> 128 x 16 x 16\n","        self.conv2a = nn.Conv2d(64,  128, kernel_size=4, padding=2 )\n","        # self.conv2b = nn.Conv2d(128, 128, kernel_size=3, padding=1 )\n","        self.pool2  = nn.MaxPool2d(2,2)\n","\n","        # block 3:         128 x 16 x 16 --> 256 x 8 x 8        \n","        self.conv3a = nn.Conv2d(128, 256, kernel_size=3, padding=1 )\n","        # self.conv3b = nn.Conv2d(256, 256, kernel_size=3, padding=1 )\n","        self.pool3  = nn.MaxPool2d(2,2)\n","        \n","        #block 4:          256 x 8 x 8 --> 512 x 4 x 4\n","        self.conv4a = nn.Conv2d(256, 512, kernel_size=3, padding=1 )\n","        self.pool4  = nn.MaxPool2d(2,2)\n","\n","        #block 5:          512 x 4 x 4 --> 512 x 2 x 2\n","        self.conv5a = nn.Conv2d(512, 512, kernel_size=3, padding=1 )\n","        self.pool5  = nn.MaxPool2d(2,2)\n","\n","        # linear layers:   512 x 2 x 2 --> 32768 --> 4096 --> 4096 --> 10\n","        self.linear1 = nn.Linear(2048, 8192)\n","        self.linear2 = nn.Linear(8192,8192)\n","        self.linear3 = nn.Linear(8192, 10)\n","\n","\n","    def forward(self, x):\n","\n","        # block 1:         3 x 32 x 32 --> 64 x 16 x 16\n","        x = self.conv1a(x)\n","        x = torch.relu(x)\n","        x = self.pool1(x)\n","\n","        # block 2:         64 x 16 x 16 --> 128 x 8 x 8\n","        x = self.conv2a(x)\n","        x = torch.relu(x)\n","\n","        x = self.pool2(x)\n","\n","        # block 3:         128 x 8 x 8 --> 256 x 4 x 4\n","        x = self.conv3a(x)\n","        x = torch.relu(x)\n","        x = self.pool3(x)\n","\n","        #block 4:          256 x 4 x 4 --> 512 x 2 x 2\n","        x = self.conv4a(x)\n","        x = torch.relu(x)\n","        x = self.pool4(x)\n","\n","        #block 5:          256 x 4 x 4 --> 256 x 2 x 2\n","        x = self.conv5a(x)\n","        x = torch.relu(x)\n","        x = self.pool5(x)\n","\n","        # linear layers:   512 x 2 x 2 --> 2048 --> 4096 --> 4096 --> 10\n","        x = x.view(-1, 2048)\n","        x = self.linear1(x)\n","        x = torch.relu(x)\n","        x = self.linear2(x)\n","        x = torch.relu(x)\n","        x = self.linear3(x) \n","        \n","        return x"],"id":"e7983848","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e798d1f0","executionInfo":{"status":"aborted","timestamp":1637237419233,"user_tz":-480,"elapsed":26,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["class LeNet5_convnet(nn.Module):\n","\n","    def __init__(self):\n","\n","        super(LeNet5_convnet, self).__init__()\n","\n","        # CL1:   3 x 64 x 64  -->    50 x 64 x 64 \n","        self.conv1 = nn.Conv2d(3,   50,  kernel_size=3,  padding=1 )\n","        \n","        # MP1: 50 x 64 x 64 -->    50 x 32 x 32\n","        self.pool1  = nn.MaxPool2d(2,2)\n","        \n","        # CL2:   50 x 32 x 32  -->    100 x 32 x 32 \n","        self.conv2 = nn.Conv2d(50,  100,  kernel_size=3,  padding=1 )\n","        \n","        # MP2: 100 x 32 x 32 -->    100 x 16 x 16\n","        self.pool2 = nn.MaxPool2d(2,2)\n","        \n","        # LL1:   100 x 7 x 7 = 4900 -->  100 \n","        self.linear1 = nn.Linear(25600, 100)\n","        \n","        # LL2:   100  -->  10 \n","        self.linear2 = nn.Linear(100,10)\n","\n","\n","    def forward(self, x):\n","\n","        # CL1:   28 x 28  -->    50 x 28 x 28 \n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        \n","        # MP1: 50 x 28 x 28 -->    50 x 14 x 14\n","        x = self.pool1(x)\n","        \n","        # CL2:   50 x 14 x 14  -->    100 x 14 x 14\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        \n","        # MP2: 100 x 14 x 14 -->    100 x 7 x 7\n","        x = self.pool2(x)\n","\n","        # LL1:   100 x 7 x 7 = 4900  -->  100 \n","        x = x.view(-1, 25600)\n","        x = self.linear1(x)\n","        x = F.relu(x)\n","        \n","        # LL2:   4900  -->  10 \n","        x = self.linear2(x)\n","    \n","        return x"],"id":"e798d1f0","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"acd55012","executionInfo":{"status":"aborted","timestamp":1637237419234,"user_tz":-480,"elapsed":27,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["mean= train_data.mean()\n","print(mean)\n","std= train_data.std()\n","print(std)"],"id":"acd55012","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9d9e02ca","executionInfo":{"status":"aborted","timestamp":1637237419234,"user_tz":-480,"elapsed":26,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["net=Vgg_convnet()\n","\n","print(net)\n","utils.display_num_param(net)"],"id":"9d9e02ca","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5aba36a2","executionInfo":{"status":"aborted","timestamp":1637237419234,"user_tz":-480,"elapsed":26,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["net = net.to(device)\n","mean = mean.to(device)\n","std = std.to(device)"],"id":"5aba36a2","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f407aebc","executionInfo":{"status":"aborted","timestamp":1637237419235,"user_tz":-480,"elapsed":27,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["criterion = nn.CrossEntropyLoss()\n","my_lr=0.01 \n","bs= 16\n","num_epoch=10"],"id":"f407aebc","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"78e8fabe","executionInfo":{"status":"aborted","timestamp":1637237419235,"user_tz":-480,"elapsed":26,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["def eval_on_test_set_cnn():\n","\n","    running_error=0\n","    num_batches=0\n","\n","    for i in range(0,100,bs):\n","\n","        minibatch_data =  test_data[i:i+bs]\n","        minibatch_label= test_label[i:i+bs]\n","\n","        minibatch_data=minibatch_data.to(device)\n","        minibatch_label=minibatch_label.to(device)\n","        \n","        inputs = (minibatch_data - mean)/std\n","        scores=net( inputs ) \n","        error = utils.get_error( scores , minibatch_label)\n","        running_error += error.item()\n","\n","        num_batches+=1\n","\n","    total_error = running_error/num_batches\n","    print( 'error rate on test set =', total_error*100 ,'percent')"],"id":"78e8fabe","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bfff9fb0","executionInfo":{"status":"aborted","timestamp":1637237419235,"user_tz":-480,"elapsed":26,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["start=time.time()\n","\n","for epoch in range(1,num_epoch):\n","    \n","    # divide the learning rate by 2 at epoch 10, 14 and 18\n","    #if epoch % 25 == 0: \n","        #my_lr = my_lr/2\n","    \n","    # create a new optimizer at the beginning of each epoch: give the current learning rate.   \n","    optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )\n","    #optimizer = torch.optim.Adam(net.parameters(), lr=my_lr)\n","    # set the running quatities to zero at the beginning of the epoch\n","    running_loss=0\n","    running_error=0\n","    num_batches=0\n","    \n","    # set the order in which to visit the image from the training set\n","    shuffled_indices=torch.randperm(979)\n"," \n","    for count in range(0,979,bs):\n","    \n","        # Set the gradients to zeros\n","        optimizer.zero_grad()\n","        \n","        # create a minibatch       \n","        indices=shuffled_indices[count:count+bs]\n","        minibatch_data =  train_data[indices]\n","        minibatch_label=  train_label[indices]\n","        \n","        # send them to the gpu\n","        minibatch_data=minibatch_data.to(device)\n","        minibatch_label=minibatch_label.to(device)  \n","\n","        inputs = (minibatch_data - mean)/std\n","        inputs.requires_grad_()\n","        scores=net( inputs )\n","        loss =  criterion( scores , minibatch_label)  \n","        loss.backward()\n","\n","        optimizer.step() \n","\n","        # START COMPUTING STATS       \n","        # add the loss of this batch to the running loss\n","        running_loss += loss.detach().item()      \n","        error = utils.get_error( scores.detach() , minibatch_label)\n","        running_error += error.item()\n","        \n","        num_batches+=1        \n","    \n","    # compute stats for the full training set\n","    total_loss = running_loss/num_batches\n","    total_error = running_error/num_batches\n","    elapsed = (time.time()-start)/60\n","    \n","    if epoch%10 == 0:\n","        print('Epoch=',epoch, '\\t time=', elapsed,'min','\\t lr=', my_lr  ,'\\t loss=', total_loss , '\\t error=', total_error*100 ,'percent')\n","        eval_on_test_set_cnn() \n","        print('-----------------------------')"],"id":"bfff9fb0","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f27b4c11"},"source":["## RNN Dog Breed Classificaiton on Gray Scale Images"],"id":"f27b4c11"},{"cell_type":"code","metadata":{"id":"511febd3","executionInfo":{"status":"aborted","timestamp":1637237419236,"user_tz":-480,"elapsed":27,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["# Hyper-parameters for RNN\n","seq_length = 64\n","input_size = 64\n","hidden_size = 256\n","num_layers = 2\n","num_classes = 10\n","bs = 50\n","learning_rate = 0.01\n","num_epochs=150"],"id":"511febd3","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"31c46efc"},"source":["### Load Gray Scale Dataset"],"id":"31c46efc"},{"cell_type":"code","metadata":{"id":"6cda164b","executionInfo":{"status":"aborted","timestamp":1637237419236,"user_tz":-480,"elapsed":27,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["train_data_gray=torch.load(dataset_folder+'train_data_gray.pt')\n","print(train_data_gray.size())\n","test_data_gray=torch.load(dataset_folder+'test_data_gray.pt')\n","print(test_data_gray.size())\n","\n","\n","train_label_gray=torch.load(dataset_folder+'train_label_gray.pt')\n","print(train_label_gray.size())\n","test_label_gray=torch.load(dataset_folder+'test_label_gray.pt')\n","print(test_label_gray.size())\n","train_dataset_gray = [{'data': train_data_gray[i], 'label': train_label_gray[i]} for i in range(len(train_data_gray)) ]\n","test_dataset_gray = [{'data': test_data_gray[i], 'label': test_label_gray[i]} for i in range(len(test_data_gray)) ]\n","\n","# Data loader\n","train_loader_gray = torch.utils.data.DataLoader(dataset=train_dataset_gray,\n","                                           batch_size=bs,\n","                                           shuffle=True)\n","test_loader_gray = torch.utils.data.DataLoader(dataset=test_dataset_gray,\n","                                          batch_size=bs,\n","                                          shuffle=False)"],"id":"6cda164b","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"65f36677","executionInfo":{"status":"aborted","timestamp":1637237419237,"user_tz":-480,"elapsed":27,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["class RNN(nn.Module):\n","    \n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","        super(RNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.layer1 = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","        self.layer2 = nn.Linear(hidden_size, num_classes)\n","\n","    \n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","        \n","\n","        out, _ = self.layer1(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n","        \n","        out = self.layer2(out[:, -1, :])\n","        return out"],"id":"65f36677","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"24ef36a4","executionInfo":{"status":"aborted","timestamp":1637237419237,"user_tz":-480,"elapsed":27,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["def eval_on_test_set_rnn(net):\n","    running_loss = 0\n","    num_batches = 0\n","    num_prediction = 0\n","    num_correct_prediction = 0\n","    \n","    net.eval()\n","    for batch_id, batch in enumerate(test_loader_gray):\n","        \n","        batch_data_num = batch[\"data\"].shape[0]\n","        num_prediction += batch_data_num\n","        \n","        minibatch_data =  batch[\"data\"].to(device)\n","        minibatch_label = batch[\"label\"].to(device)\n","                                  \n","        output  = net( minibatch_data)\n","        predicted_label = torch.argmax(output, dim=1)\n","        num_correct_prediction += torch.sum(minibatch_label == predicted_label).item()\n","                \n","        loss = criterion(  output ,  minibatch_label )    \n","\n","        running_loss += loss.item()\n","        num_batches += 1        \n","    \n","    total_loss = running_loss/num_batches \n","    accuracy = num_correct_prediction / num_prediction\n","    print(f'Test: exp(loss) = {math.exp(total_loss):.4f}\\tTest accuracy = {(accuracy*100):.4f}')"],"id":"24ef36a4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6f5c255a","executionInfo":{"status":"aborted","timestamp":1637237419237,"user_tz":-480,"elapsed":25,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["hidden_size = 256\n","num_layers = 2\n","net=RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"],"id":"6f5c255a","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"83c187bf","executionInfo":{"status":"aborted","timestamp":1637237419238,"user_tz":-480,"elapsed":26,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["start=time.time()\n","for epoch in range(num_epochs):\n","    \n","    # divide the learning rate by 3 except after the first epoch\n","#     if epoch >= 2:\n","#         learning_rate = learning_rate / 3\n","    \n","    # create a new optimizer at the beginning of each epoch: give the current learning rate.   \n","    optimizer=torch.optim.SGD( net.parameters() , lr=learning_rate )\n","\n","    # set the running quatities to zero at the beginning of the epoch\n","    running_loss=0\n","    num_batches=0    \n","    net.train()\n","    num_prediction = 0\n","    num_correct_prediction = 0\n","    for batch_id, batch in enumerate(train_loader_gray):\n","        \n","        # Set the gradients to zeros\n","        optimizer.zero_grad()\n","        batch_data_num = batch[\"data\"].shape[0]\n","        num_prediction += batch_data_num\n","        \n","        # create a minibatch\n","        # send them to the gpu\n","        minibatch_data =  batch[\"data\"].to(device)\n","        minibatch_label = batch[\"label\"].to(device)\n","        \n","        # forward the minibatch through the net        \n","        output  = net( minibatch_data)\n","        predicted_label = torch.argmax(output, dim = 1)\n","        num_correct_prediction += torch.sum(minibatch_label == predicted_label).item()\n","        \n","        # Compute the average of the losses of the data points in this huge batch\n","        loss = criterion(  output ,  minibatch_label )\n","        loss.backward()\n","\n","        # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n","        utils.normalize_gradient(net)\n","        optimizer.step()\n","        \n","        # update the running loss  \n","        running_loss += loss.item()\n","        num_batches += 1\n","        \n","    accuracy = num_correct_prediction / num_prediction\n","    # compute stats for the full training set\n","    total_loss = running_loss / num_batches\n","    elapsed = time.time() - start\n","    \n","    if epoch%10 == 0:\n","        print(f'Epoch {epoch} \\ttime = {elapsed:.3f}\\tlr = {learning_rate}\\nTrain: exp(loss) = {math.exp(total_loss):.4f}\\tTrain accuracy = {(accuracy*100):.4f}')\n","        eval_on_test_set_rnn(net)\n","        print(f'-----------------------------------')"],"id":"83c187bf","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Xe3QdWQ-h3O","executionInfo":{"status":"ok","timestamp":1637239461531,"user_tz":-480,"elapsed":392,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}},"outputId":"0386e8c1-ada3-44d5-ca56-6035cfc9faff"},"source":["# # for testing\n","# if 'google.colab' in sys.modules:\n","#     # mount google drive\n","#     from google.colab import drive\n","#     drive.mount('/content/gdrive')\n","#     # file_name = 'dataset_v1.1.zip'\n","# path_to_file = \"/content/gdrive/MyDrive/dog_data\"\n","# train_data=torch.load(path_to_file+'/train_data_final.pt')\n","# train_label=torch.load(path_to_file+'/train_label_final.pt')\n","# test_data=torch.load(path_to_file+'/test_data_final.pt')\n","# test_label=torch.load(path_to_file+'/test_label_final.pt')"],"id":"8Xe3QdWQ-h3O","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"NW3_FMI2BL0e"},"source":["## Transfer Learning using ResNet\n","Since the performance of our own built MLP and CNN models do not meet our expection (30%~50% error rate), we explore other techniques to improve the accuracy. Transfer learning which focuses on storing knowledge gained while solving one problem and applying it to a different but related problem comes to our minds. We have tested pre-trained models from ResNet and the performance is around."],"id":"NW3_FMI2BL0e"},{"cell_type":"code","metadata":{"id":"gKOlfRezCfy4","executionInfo":{"status":"ok","timestamp":1637239470643,"user_tz":-480,"elapsed":455,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["def collate_fn(batch):\n","  data = [x['data'] for x in batch]\n","  label = [x['label'] for x in batch]\n","  data = torch.stack(data, dim=0)\n","  label = torch.stack(label)\n","  return (data, label)"],"id":"gKOlfRezCfy4","execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"IytASOjS-iDo","executionInfo":{"status":"ok","timestamp":1637239480179,"user_tz":-480,"elapsed":309,"user":{"displayName":"Zhanhua Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64","userId":"12560135567688846267"}}},"source":["# prepare dataloader\n","train_dataset = [{'data': train_data[i], 'label': train_label[i]} for i in range(len(train_data)) ]\n","test_dataset = [{'data': test_data[i], 'label': test_label[i]} for i in range(len(test_data)) ]\n","# image_datasets = {'train': [train_data, train_label], 'val': [test_data, test_label]}\n","# dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'val']}\n","train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn = collate_fn)\n","test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn = collate_fn)\n","dataloaders = {'train': train_loader, 'val':test_loader}\n","dataset_sizes = {'train': len(train_dataset), 'val':len(test_dataset)}"],"id":"IytASOjS-iDo","execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"cz0YZnbw-iJj"},"source":["device= torch.device(\"cuda\")\n","model = models.resnet50(pretrained=True).to(device)\n","\n","for param in model.parameters():\n","    param.requires_grad = False   \n","    \n","# model.fc = nn.Sequential(nn.Linear(2048, 256), nn.ReLU(inplace=True), nn.Linear(256, 10)).to(device)\n","# model.fc = nn.Sequential(nn.Linear(2048, 256), torch.nn.Tanh(), nn.Linear(256, 10)).to(device)\n","model.fc = nn.Sequential(nn.Linear(2048, 256), nn.Dropout(0.1), nn.Linear(256, 10)).to(device)"],"id":"cz0YZnbw-iJj","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qaQi4yg5Cxs0"},"source":["def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n","    since = time.time()\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","            if phase == 'train':\n","                scheduler.step()\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","            current_lr = optimizer.param_groups[0]['lr']\n","            print('{} Loss: {:.4f} Acc: {:.4f} Current LR: {:.4f}'.format(\n","                phase, epoch_loss, epoch_acc, current_lr))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model"],"id":"qaQi4yg5Cxs0","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BzP6PwoV-iL-"},"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"],"id":"BzP6PwoV-iL-","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"86b05c42"},"source":["model_trained = train_model(model, criterion, optimizer, scheduler, num_epochs=100)"],"id":"86b05c42","execution_count":null,"outputs":[]}]}