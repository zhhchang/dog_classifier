{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dog Breed Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Initially, we collected more than 3,000 dog images consist of more than 30 breeds. 10 breeds are selected to be containded in the final dataset.  The train folder contains 1,179 images of dogs. Each image in the folder has the breed label and a numeric id as part of the filename. The test folder contains 100 images.\n",
    "For each image in the train and test set, the distribution of each dog breeds are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Fe0pvmUVtfFd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from random import randint\n",
    "import time\n",
    "import utils\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-kw0q0xjFEyD",
    "outputId": "23c70ac2-6416-4da8-8d1c-140f6ded8c00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device= torch.device(\"cuda\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset from pt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PwTkTV0Twjc-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([979, 3, 64, 64])\n",
      "torch.Size([100, 3, 64, 64])\n",
      "torch.Size([979])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "dataset_folder=''\n",
    "train_data=torch.load(dataset_folder+'train_data_final.pt')\n",
    "print(train_data.size())\n",
    "\n",
    "test_data=torch.load(dataset_folder+'test_data_final.pt')\n",
    "print(test_data.size())\n",
    "\n",
    "train_label=torch.load(dataset_folder+'train_label_final.pt')\n",
    "print(train_label.size())\n",
    "\n",
    "test_label=torch.load(dataset_folder+'test_label_final.pt')\n",
    "print(test_label.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AbnjSf1K2WJV",
    "outputId": "65796ad8-7588-4e27-db18-ebc619958d5a"
   },
   "source": [
    "## Explore the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_label_list=train_label.tolist()\n",
    "#test_label_list\n",
    "df_train_label = pd.DataFrame(test_label_list)\n",
    "column_name = [\"label\"]\n",
    "df_train_label.columns = [\"label\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def plot_image_list_count(df_train_label):\n",
    "    seaborn.countplot(label)\n",
    "    plt.title('Cats and Dogs')\n",
    "    \n",
    "plot_image_list_count(df_train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Layers MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class three_layer_net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3,output_size):\n",
    "        super(three_layer_net , self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(  input_size   , hidden_size1  , bias=False  )\n",
    "        self.layer2 = nn.Linear(  hidden_size1 , hidden_size2  , bias=False  )\n",
    "        self.layer3 = nn.Linear(  hidden_size2 , hidden_size3  , bias=False  )\n",
    "        self.layer4 = nn.Linear(  hidden_size3 , output_size   , bias=False  )        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        y       = self.layer1(x)\n",
    "        y_hat   = torch.relu(y)\n",
    "        z       = self.layer2(y_hat)\n",
    "        z_hat   = torch.relu(z)\n",
    "        a       = self.layer3(z_hat)\n",
    "        a_hat   = torch.relu(a)\n",
    "        scores  = self.layer4(a_hat)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three_layer_net(\n",
      "  (layer1): Linear(in_features=12288, out_features=500, bias=False)\n",
      "  (layer2): Linear(in_features=500, out_features=500, bias=False)\n",
      "  (layer3): Linear(in_features=500, out_features=500, bias=False)\n",
      "  (layer4): Linear(in_features=500, out_features=11, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net=three_layer_net(12288,500,500,500,11)\n",
    "print(net)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD( net.parameters() , lr=0.01 )\n",
    "bs= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_test_set_mlp():\n",
    "\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "\n",
    "\n",
    "    for i in range(0,100,bs):\n",
    "\n",
    "        # extract the minibatch\n",
    "        minibatch_data =  test_data[i:i+bs]\n",
    "        minibatch_label= test_label[i:i+bs]\n",
    "        # send them to the gpu\n",
    "        #minibatch_data=minibatch_data.to(device)\n",
    "        #minibatch_label=minibatch_label.to(device)\n",
    "\n",
    "        # reshape the minibatch\n",
    "        inputs = minibatch_data.view(bs,12288)\n",
    "\n",
    "        # feed it to the network\n",
    "        scores=net( inputs ) \n",
    "\n",
    "        # compute the error made on this batch\n",
    "        error = utils.get_error( scores , minibatch_label)\n",
    "\n",
    "        # add it to the running error\n",
    "        running_error += error.item()\n",
    "\n",
    "        num_batches+=1\n",
    "\n",
    "\n",
    "    # compute error rate on the full test set\n",
    "    total_error = running_error/num_batches\n",
    "\n",
    "    print( 'error rate on test set =', total_error*100 ,'percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0 \t time= 0.5229840278625488 \t loss= 2.3628940238166103 \t error= 85.2577318235771 percent\n",
      "error rate on test set = 88.99999856948853 percent\n",
      " \n",
      "epoch= 1 \t time= 0.9450070858001709 \t loss= 2.2780536745012423 \t error= 83.19587590768165 percent\n",
      "error rate on test set = 80.0 percent\n",
      " \n",
      "epoch= 2 \t time= 1.3466553688049316 \t loss= 2.1638254222181654 \t error= 80.8247417518773 percent\n",
      "error rate on test set = 80.0 percent\n",
      " \n",
      "epoch= 3 \t time= 1.7558119297027588 \t loss= 2.0949826326566874 \t error= 79.07216469037164 percent\n",
      "error rate on test set = 73.99999976158142 percent\n",
      " \n",
      "epoch= 4 \t time= 2.1461446285247803 \t loss= 2.052941962615731 \t error= 77.83505117770324 percent\n",
      "error rate on test set = 69.99999940395355 percent\n",
      " \n",
      "epoch= 5 \t time= 2.5286316871643066 \t loss= 2.0159183691457376 \t error= 75.25773183586672 percent\n",
      "error rate on test set = 70.99999964237213 percent\n",
      " \n",
      "epoch= 6 \t time= 2.935267925262451 \t loss= 1.9890743838143103 \t error= 74.43298955553585 percent\n",
      "error rate on test set = 72.00000047683716 percent\n",
      " \n",
      "epoch= 7 \t time= 3.2826833724975586 \t loss= 1.956386665708011 \t error= 73.19587653445214 percent\n",
      "error rate on test set = 66.99999988079071 percent\n",
      " \n",
      "epoch= 8 \t time= 3.6400060653686523 \t loss= 1.9206884583247077 \t error= 72.57731944015346 percent\n",
      "error rate on test set = 73.99999976158142 percent\n",
      " \n",
      "epoch= 9 \t time= 4.003324747085571 \t loss= 1.8848784031327237 \t error= 69.58762905032364 percent\n",
      "error rate on test set = 75.99999964237213 percent\n",
      " \n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    running_loss=0\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "    \n",
    "    shuffled_indices=torch.randperm(970)\n",
    " \n",
    "    for count in range(0,970,bs):\n",
    "    \n",
    "        # Set the gradients to zeros\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # create a minibatch       \n",
    "        indices=shuffled_indices[count:count+bs]\n",
    "        minibatch_data =  train_data[indices]\n",
    "        minibatch_label=  train_label[indices]\n",
    "        \n",
    "        # send them to the gpu\n",
    "        #minibatch_data=minibatch_data.to(device)\n",
    "        #minibatch_label=minibatch_label.to(device)\n",
    "        \n",
    "        # reshape the minibatch\n",
    "        inputs = minibatch_data.view(bs,12288)\n",
    "\n",
    "        # tell Pytorch to start tracking all operations that will be done on \"inputs\"\n",
    "        inputs.requires_grad_()\n",
    "\n",
    "        # forward the minibatch through the net \n",
    "        scores=net( inputs ) \n",
    "\n",
    "        # Compute the average of the losses of the data points in the minibatch\n",
    "        loss =  criterion( scores , minibatch_label) \n",
    "        \n",
    "        # backward pass to compute dL/dU, dL/dV and dL/dW   \n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent: U=U-lr(dL/dU), V=V-lr(dL/dU), ...\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        # START COMPUTING STATS\n",
    "        \n",
    "        # add the loss of this batch to the running loss\n",
    "        running_loss += loss.detach().item()\n",
    "        \n",
    "        # compute the error made on this batch and add it to the running error       \n",
    "        error = utils.get_error( scores.detach() , minibatch_label)\n",
    "        running_error += error.item()\n",
    "        \n",
    "        num_batches+=1        \n",
    "    \n",
    "    \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss/num_batches\n",
    "    total_error = running_error/num_batches\n",
    "    elapsed = time.time()-start\n",
    "    \n",
    "#if epoch%2 == 0:\n",
    "    print('epoch=',epoch, '\\t time=', elapsed, '\\t loss=', total_loss , '\\t error=', total_error*100 ,'percent')\n",
    "    eval_on_test_set_mlp() \n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a CNN for Dog Breed Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Ia1ZXd5M75aq"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # block 1:         3 x 64 x 64 --> 64 x 32 x 32        \n",
    "        self.conv1a = nn.Conv2d(3,   64,  kernel_size=8, padding=4 )\n",
    "        # self.conv1b = nn.Conv2d(64,  64,  kernel_size=3, padding=1 )\n",
    "        self.pool1  = nn.MaxPool2d(2,2)\n",
    "\n",
    "        # block 2:         64 x 32 x 32 --> 128 x 16 x 16\n",
    "        self.conv2a = nn.Conv2d(64,  128, kernel_size=4, padding=2 )\n",
    "        # self.conv2b = nn.Conv2d(128, 128, kernel_size=3, padding=1 )\n",
    "        self.pool2  = nn.MaxPool2d(2,2)\n",
    "\n",
    "        # block 3:         128 x 16 x 16 --> 256 x 8 x 8        \n",
    "        self.conv3a = nn.Conv2d(128, 256, kernel_size=3, padding=1 )\n",
    "        # self.conv3b = nn.Conv2d(256, 256, kernel_size=3, padding=1 )\n",
    "        self.pool3  = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        #block 4:          256 x 8 x 8 --> 512 x 4 x 4\n",
    "        self.conv4a = nn.Conv2d(256, 512, kernel_size=3, padding=1 )\n",
    "        self.pool4  = nn.MaxPool2d(2,2)\n",
    "\n",
    "        #block 5:          512 x 4 x 4 --> 512 x 2 x 2\n",
    "        self.conv5a = nn.Conv2d(512, 512, kernel_size=3, padding=1 )\n",
    "        self.pool5  = nn.MaxPool2d(2,2)\n",
    "\n",
    "        # linear layers:   512 x 2 x 2 --> 32768 --> 4096 --> 4096 --> 10\n",
    "        self.linear1 = nn.Linear(2048, 8192)\n",
    "        self.linear2 = nn.Linear(8192,8192)\n",
    "        self.linear3 = nn.Linear(8192, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # block 1:         3 x 32 x 32 --> 64 x 16 x 16\n",
    "        x = self.conv1a(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # block 2:         64 x 16 x 16 --> 128 x 8 x 8\n",
    "        x = self.conv2a(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # block 3:         128 x 8 x 8 --> 256 x 4 x 4\n",
    "        x = self.conv3a(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        #block 4:          256 x 4 x 4 --> 512 x 2 x 2\n",
    "        x = self.conv4a(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        #block 5:          256 x 4 x 4 --> 256 x 2 x 2\n",
    "        x = self.conv5a(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool5(x)\n",
    "\n",
    "        # linear layers:   512 x 2 x 2 --> 2048 --> 4096 --> 4096 --> 10\n",
    "        x = x.view(-1, 2048)\n",
    "        x = self.linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.linear3(x) \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l_pBLdBOEPJ_",
    "outputId": "e092715a-b230-4d21-a64c-e36ae60078b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4272)\n",
      "tensor(0.2518)\n"
     ]
    }
   ],
   "source": [
    "mean= train_data.mean()\n",
    "print(mean)\n",
    "std= train_data.std()\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ynLviVkEERkT",
    "outputId": "2cf9adeb-eceb-4d36-e2a7-ffdc6d1b7b54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1a): Conv2d(3, 64, kernel_size=(8, 8), stride=(1, 1), padding=(4, 4))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2a): Conv2d(64, 128, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3a): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4a): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv5a): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "  (linear2): Linear(in_features=8192, out_features=8192, bias=True)\n",
      "  (linear3): Linear(in_features=8192, out_features=10, bias=True)\n",
      ")\n",
      "There are 87963082 (87.96 million) parameters in this neural network\n"
     ]
    }
   ],
   "source": [
    "net=CNN()\n",
    "\n",
    "print(net)\n",
    "utils.display_num_param(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "NHEwxbEnJDPE"
   },
   "outputs": [],
   "source": [
    "net = net.to(device)\n",
    "mean = mean.to(device)\n",
    "std = std.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "EB4DK8mvETY5"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "my_lr=0.01 \n",
    "bs= 16\n",
    "num_epoch=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5W4sL3CtEVws"
   },
   "outputs": [],
   "source": [
    "def eval_on_test_set_cnn():\n",
    "\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "\n",
    "    for i in range(0,100,bs):\n",
    "\n",
    "        minibatch_data =  test_data[i:i+bs]\n",
    "        minibatch_label= test_label[i:i+bs]\n",
    "\n",
    "        minibatch_data=minibatch_data.to(device)\n",
    "        minibatch_label=minibatch_label.to(device)\n",
    "        \n",
    "        inputs = (minibatch_data - mean)/std\n",
    "        scores=net( inputs ) \n",
    "        error = utils.get_error( scores , minibatch_label)\n",
    "        running_error += error.item()\n",
    "\n",
    "        num_batches+=1\n",
    "\n",
    "    total_error = running_error/num_batches\n",
    "    print( 'error rate on test set =', total_error*100 ,'percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "I-2AJ7pYEXxD",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "\n",
    "for epoch in range(1,num_epoch):\n",
    "    \n",
    "    # divide the learning rate by 2 at epoch 10, 14 and 18\n",
    "    #if epoch % 25 == 0: \n",
    "        #my_lr = my_lr/2\n",
    "    \n",
    "    # create a new optimizer at the beginning of each epoch: give the current learning rate.   \n",
    "    optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )\n",
    "    #optimizer = torch.optim.Adam(net.parameters(), lr=my_lr)\n",
    "    # set the running quatities to zero at the beginning of the epoch\n",
    "    running_loss=0\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "    \n",
    "    # set the order in which to visit the image from the training set\n",
    "    shuffled_indices=torch.randperm(979)\n",
    " \n",
    "    for count in range(0,979,bs):\n",
    "    \n",
    "        # Set the gradients to zeros\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # create a minibatch       \n",
    "        indices=shuffled_indices[count:count+bs]\n",
    "        minibatch_data =  train_data[indices]\n",
    "        minibatch_label=  train_label[indices]\n",
    "        \n",
    "        # send them to the gpu\n",
    "        minibatch_data=minibatch_data.to(device)\n",
    "        minibatch_label=minibatch_label.to(device)  \n",
    "\n",
    "        inputs = (minibatch_data - mean)/std\n",
    "        inputs.requires_grad_()\n",
    "        scores=net( inputs )\n",
    "        loss =  criterion( scores , minibatch_label)  \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step() \n",
    "\n",
    "        # START COMPUTING STATS       \n",
    "        # add the loss of this batch to the running loss\n",
    "        running_loss += loss.detach().item()      \n",
    "        error = utils.get_error( scores.detach() , minibatch_label)\n",
    "        running_error += error.item()\n",
    "        \n",
    "        num_batches+=1        \n",
    "    \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss/num_batches\n",
    "    total_error = running_error/num_batches\n",
    "    elapsed = (time.time()-start)/60\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        print('Epoch=',epoch, '\\t time=', elapsed,'min','\\t lr=', my_lr  ,'\\t loss=', total_loss , '\\t error=', total_error*100 ,'percent')\n",
    "        eval_on_test_set_cnn() \n",
    "        print('-----------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Dog Breed Classificaiton on Gray Scale Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters for RNN\n",
    "seq_length = 64\n",
    "input_size = 64\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "bs = 50\n",
    "learning_rate = 0.01\n",
    "num_epochs=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([979, 64, 64])\n",
      "torch.Size([100, 64, 64])\n",
      "torch.Size([979])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "train_data=torch.load(dataset_folder+'train_data_gray.pt')\n",
    "print(train_data.size())\n",
    "test_data=torch.load(dataset_folder+'test_data_gray.pt')\n",
    "print(test_data.size())\n",
    "\n",
    "\n",
    "train_label=torch.load(dataset_folder+'train_label_gray.pt')\n",
    "print(train_label.size())\n",
    "test_label=torch.load(dataset_folder+'test_label_gray.pt')\n",
    "print(test_label.size())\n",
    "train_dataset = [{'data': train_data[i], 'label': train_label[i]} for i in range(len(train_data)) ]\n",
    "test_dataset = [{'data': test_data[i], 'label': test_label[i]} for i in range(len(test_data)) ]\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=bs,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=bs,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent neural network (many-to-one)\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.layer1 = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.layer2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "\n",
    "        out, _ = self.layer1(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        out = self.layer2(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_test_set_rnn(net):\n",
    "    running_loss = 0\n",
    "    num_batches = 0\n",
    "    num_prediction = 0\n",
    "    num_correct_prediction = 0\n",
    "    \n",
    "    net.eval()\n",
    "    for batch_id, batch in enumerate(test_loader):\n",
    "        \n",
    "        batch_data_num = batch[\"data\"].shape[0]\n",
    "        num_prediction += batch_data_num\n",
    "        \n",
    "        minibatch_data =  batch[\"data\"].to(device)\n",
    "        minibatch_label = batch[\"label\"].to(device)\n",
    "                                  \n",
    "        output  = net( minibatch_data)\n",
    "        predicted_label = torch.argmax(output, dim=1)\n",
    "        num_correct_prediction += torch.sum(minibatch_label == predicted_label).item()\n",
    "                \n",
    "        loss = criterion(  output ,  minibatch_label )    \n",
    "\n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1        \n",
    "    \n",
    "    total_loss = running_loss/num_batches \n",
    "    accuracy = num_correct_prediction / num_prediction\n",
    "    print(f'Test: exp(loss) = {math.exp(total_loss):.4f}\\tTest accuracy = {(accuracy*100):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "net=RNN(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 150\ttime = 0.312\tlr = 0.01\n",
      "Train: exp(loss) = 10.0121\tTrain accuracy = 8.2737\n",
      "Test: exp(loss) = 9.9882\tTest accuracy = 10.0000\n",
      "-----------------------------------\n",
      "Epoch 10 / 150\ttime = 3.600\tlr = 0.01\n",
      "Train: exp(loss) = 9.9144\tTrain accuracy = 11.9510\n",
      "Test: exp(loss) = 9.9650\tTest accuracy = 11.0000\n",
      "-----------------------------------\n",
      "Epoch 20 / 150\ttime = 6.848\tlr = 0.01\n",
      "Train: exp(loss) = 9.8420\tTrain accuracy = 13.5853\n",
      "Test: exp(loss) = 9.8909\tTest accuracy = 14.0000\n",
      "-----------------------------------\n",
      "Epoch 30 / 150\ttime = 10.405\tlr = 0.01\n",
      "Train: exp(loss) = 9.7014\tTrain accuracy = 17.4668\n",
      "Test: exp(loss) = 9.6654\tTest accuracy = 19.0000\n",
      "-----------------------------------\n",
      "Epoch 40 / 150\ttime = 14.128\tlr = 0.01\n",
      "Train: exp(loss) = 9.0744\tTrain accuracy = 20.2247\n",
      "Test: exp(loss) = 8.7461\tTest accuracy = 23.0000\n",
      "-----------------------------------\n",
      "Epoch 50 / 150\ttime = 17.491\tlr = 0.01\n",
      "Train: exp(loss) = 8.2742\tTrain accuracy = 21.7569\n",
      "Test: exp(loss) = 7.9707\tTest accuracy = 24.0000\n",
      "-----------------------------------\n",
      "Epoch 60 / 150\ttime = 20.747\tlr = 0.01\n",
      "Train: exp(loss) = 8.0380\tTrain accuracy = 23.0848\n",
      "Test: exp(loss) = 7.7631\tTest accuracy = 25.0000\n",
      "-----------------------------------\n",
      "Epoch 70 / 150\ttime = 23.990\tlr = 0.01\n",
      "Train: exp(loss) = 7.8856\tTrain accuracy = 21.7569\n",
      "Test: exp(loss) = 7.5013\tTest accuracy = 28.0000\n",
      "-----------------------------------\n",
      "Epoch 80 / 150\ttime = 27.248\tlr = 0.01\n",
      "Train: exp(loss) = 7.8712\tTrain accuracy = 22.1655\n",
      "Test: exp(loss) = 7.4398\tTest accuracy = 29.0000\n",
      "-----------------------------------\n",
      "Epoch 90 / 150\ttime = 30.489\tlr = 0.01\n",
      "Train: exp(loss) = 7.7722\tTrain accuracy = 21.8590\n",
      "Test: exp(loss) = 7.4381\tTest accuracy = 25.0000\n",
      "-----------------------------------\n",
      "Epoch 100 / 150\ttime = 33.735\tlr = 0.01\n",
      "Train: exp(loss) = 7.6939\tTrain accuracy = 21.8590\n",
      "Test: exp(loss) = 7.4816\tTest accuracy = 28.0000\n",
      "-----------------------------------\n",
      "Epoch 110 / 150\ttime = 36.991\tlr = 0.01\n",
      "Train: exp(loss) = 7.6627\tTrain accuracy = 22.0633\n",
      "Test: exp(loss) = 7.3947\tTest accuracy = 28.0000\n",
      "-----------------------------------\n",
      "Epoch 120 / 150\ttime = 40.238\tlr = 0.01\n",
      "Train: exp(loss) = 7.5939\tTrain accuracy = 22.7783\n",
      "Test: exp(loss) = 7.2839\tTest accuracy = 26.0000\n",
      "-----------------------------------\n",
      "Epoch 130 / 150\ttime = 43.493\tlr = 0.01\n",
      "Train: exp(loss) = 7.5570\tTrain accuracy = 22.7783\n",
      "Test: exp(loss) = 7.4192\tTest accuracy = 25.0000\n",
      "-----------------------------------\n",
      "Epoch 140 / 150\ttime = 46.744\tlr = 0.01\n",
      "Train: exp(loss) = 7.4886\tTrain accuracy = 23.0848\n",
      "Test: exp(loss) = 7.2732\tTest accuracy = 27.0000\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # divide the learning rate by 3 except after the first epoch\n",
    "#     if epoch >= 2:\n",
    "#         learning_rate = learning_rate / 3\n",
    "    \n",
    "    # create a new optimizer at the beginning of each epoch: give the current learning rate.   \n",
    "    optimizer=torch.optim.SGD( net.parameters() , lr=learning_rate )\n",
    "\n",
    "    # set the running quatities to zero at the beginning of the epoch\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "    net.train()\n",
    "    num_prediction = 0\n",
    "    num_correct_prediction = 0\n",
    "    for batch_id, batch in enumerate(train_loader):\n",
    "        \n",
    "        # Set the gradients to zeros\n",
    "        optimizer.zero_grad()\n",
    "        batch_data_num = batch[\"data\"].shape[0]\n",
    "        num_prediction += batch_data_num\n",
    "        \n",
    "        # create a minibatch\n",
    "        # send them to the gpu\n",
    "        minibatch_data =  batch[\"data\"].to(device)\n",
    "        minibatch_label = batch[\"label\"].to(device)\n",
    "        \n",
    "        # forward the minibatch through the net        \n",
    "        output  = net( minibatch_data)\n",
    "        predicted_label = torch.argmax(output, dim = 1)\n",
    "        num_correct_prediction += torch.sum(minibatch_label == predicted_label).item()\n",
    "        \n",
    "        # Compute the average of the losses of the data points in this huge batch\n",
    "        loss = criterion(  output ,  minibatch_label )\n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
    "        utils.normalize_gradient(net)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update the running loss  \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "    accuracy = num_correct_prediction / num_prediction\n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss / num_batches\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        print(f'Epoch {epoch} / {num_epochs}\\ttime = {elapsed:.3f}\\tlr = {learning_rate}\\nTrain: exp(loss) = {math.exp(total_loss):.4f}\\tTrain accuracy = {(accuracy*100):.4f}')\n",
    "        eval_on_test_set_rnn(net)\n",
    "        print(f'-----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN-colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
