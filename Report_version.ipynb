{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7e4df1d7",
   "metadata": {
    "executionInfo": {
     "elapsed": 1875,
     "status": "ok",
     "timestamp": 1637239447105,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "7e4df1d7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from random import randint\n",
    "import time\n",
    "import os\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision.transforms import transforms\n",
    "import requests\n",
    "from os.path import dirname, exists\n",
    "from os import makedirs\n",
    "import pandas as pd\n",
    "import math\n",
    "import sys\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.optim import lr_scheduler\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b86828f1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "executionInfo": {
     "elapsed": 380,
     "status": "error",
     "timestamp": 1637238237617,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "b86828f1",
    "outputId": "5606eb70-95f8-4e85-b691-804c66648f50"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7b291dbe4a42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# rgb dataset .pt path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../dog_dataset/train_data_final.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../dog_dataset/train_label_final.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../dog_dataset/test_data_final.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../dog_dataset/train_data_final.pt'"
     ]
    }
   ],
   "source": [
    "# dataset image path\n",
    "train_img_path = '../dog_dataset/train_64x64'\n",
    "test_img_path = '../dog_dataset/test_64x64'\n",
    "\n",
    "# rgb dataset .pt path\n",
    "train_data=torch.load('../dog_dataset/train_data_final.pt')\n",
    "train_label=torch.load('../dog_dataset/train_label_final.pt')\n",
    "test_data=torch.load('../dog_dataset/test_data_final.pt')\n",
    "test_label=torch.load('../dog_dataset/test_label_final.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b556d4df",
   "metadata": {
    "id": "b556d4df"
   },
   "source": [
    "# Dataset Collection #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2f267c",
   "metadata": {
    "id": "2b2f267c"
   },
   "source": [
    "Generate scraper to download the images from https://dog.ceo/dog-api/ for all the breeds to form our raw dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b7d822",
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "aborted",
     "timestamp": 1637237419224,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "74b7d822"
   },
   "outputs": [],
   "source": [
    "project_path = dirname(__file__)\n",
    "data_dir = project_path + \"/data\"\n",
    "if not exists(data_dir):\n",
    "    makedirs(data_dir)\n",
    "\n",
    "\n",
    "breed_url = \"https://dog.ceo/api/breeds/list/all\"\n",
    "breed_response = requests.get(breed_url)\n",
    "breed_dict = breed_response.json().get('message')\n",
    "single_breed = [x for x in breed_dict if len(breed_dict.get(x)) == 0]\n",
    "multi_breed = [i+\"/\"+j for i in breed_dict for j in breed_dict.get(i) if len(breed_dict.get(i)) > 0]\n",
    "breed_lst = single_breed + multi_breed\n",
    "\n",
    "\n",
    "def get_img_url(breed):\n",
    "    url = f\"https://dog.ceo/api/breed/{breed}/images\"\n",
    "    img_response = requests.get(url)\n",
    "    img_lst = img_response.json().get('message')\n",
    "    if not exists(f\"{data_dir}/{breed}\"):\n",
    "        makedirs(f\"{data_dir}/{breed}\")\n",
    "    for i in img_lst:\n",
    "        download_img(breed, i)\n",
    "\n",
    "\n",
    "def download_img(breed, url):\n",
    "    response = requests.get(url)\n",
    "    with open(f\"{data_dir}/{breed}/{url.split('/')[-1]}\", \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "\n",
    "for i in breed_lst:\n",
    "    get_img_url(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e911256",
   "metadata": {
    "id": "8e911256"
   },
   "source": [
    "# Exploratory Data Analysis And Data Preprocessing #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9734129",
   "metadata": {
    "id": "d9734129"
   },
   "source": [
    "## First Look of the Raw Dataset ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8923a0a0",
   "metadata": {
    "id": "8923a0a0"
   },
   "source": [
    "A glance of the raw dataset downloaded from Dog API using the scraper \n",
    "<div>\n",
    "    <img src=\"report_attachment/raw_data_look.png\" width='500' align='left'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fda93e",
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "aborted",
     "timestamp": 1637237419225,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "a2fda93e"
   },
   "outputs": [],
   "source": [
    "raw_data_path = '../data'\n",
    "print('Total number of breeds in the raw dataset: ', len([i for i in os.listdir(raw_data_path)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2a1fc3",
   "metadata": {
    "id": "2c2a1fc3"
   },
   "source": [
    "## Dataset Preperation ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e886b9f",
   "metadata": {
    "id": "3e886b9f"
   },
   "source": [
    "There are in total 91 dog breeds in the raw dataset.  \n",
    "However, the data sizes for each breed are varing from a dozen to more than a hundred.  \n",
    "To create a balanced dataset, we have filtered **10 most popular dog breeds** out of the list and with data size **more than 100 images** .  \n",
    "Below is the list of dog breeds to be classified in our classsifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6295c485",
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "aborted",
     "timestamp": 1637237419225,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "6295c485"
   },
   "outputs": [],
   "source": [
    "# list of dog breeds to be classified in our dog classifier\n",
    "dog_dict = {\n",
    "    'affenpinscher':0,\n",
    "    'beagle':1,\n",
    "    'boxer':2,\n",
    "    'chihuahua':3,\n",
    "    'frenchbulldog':4,\n",
    "    'goldenretriever':5,\n",
    "    'rottweiler':6,\n",
    "    'schnauzer':7,\n",
    "    'sheepdog':8,\n",
    "    'spaniel':9\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a6afa5",
   "metadata": {
    "id": "a3a6afa5"
   },
   "source": [
    "To improve the quality of the dataset, we screen through the images and unsuitable images are removed from the dataset, e.g., images with vague objects.  \n",
    "After data cleaning, the dataset is seperated into 2 sets: train set and test set with balanced class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9171e847",
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "aborted",
     "timestamp": 1637237419226,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "9171e847"
   },
   "outputs": [],
   "source": [
    "print('train set size: ', train_data.size()[0])\n",
    "print('test set size: ', test_data.size()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2e8469",
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "aborted",
     "timestamp": 1637237419227,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "2d2e8469"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "# train data class distribution\n",
    "ax1 = fig.add_axes([0, 0, 1, 1], aspect=1)\n",
    "plt.title(\"train data\")\n",
    "data = train_label.numpy()\n",
    "data = pd.Series(data).value_counts()\n",
    "label_num = data.index.tolist()\n",
    "label = []\n",
    "for i in range(len(label_num)):\n",
    "    label.append([k for k, v in dog_dict.items() if v == label_num[i]])\n",
    "label = np.asarray(label).squeeze()\n",
    "ax1.pie(data, labels = label, autopct='%.2f%%')\n",
    "\n",
    "# test data class distribution\n",
    "ax2 = fig.add_axes([1, .0, 1, 1], aspect=1)\n",
    "plt.title(\"test data\")\n",
    "data = test_label.numpy()\n",
    "data = pd.Series(data).value_counts()\n",
    "label_num = data.index.tolist()\n",
    "label = []\n",
    "for i in range(len(label_num)):\n",
    "    label.append([k for k, v in dog_dict.items() if v == label_num[i]])\n",
    "label = np.asarray(label).squeeze()\n",
    "ax2.pie(data, labels = label, autopct='%.2f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff85ffcd",
   "metadata": {
    "id": "ff85ffcd"
   },
   "source": [
    "## Data Preprocessing ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d885dd61",
   "metadata": {
    "id": "d885dd61"
   },
   "source": [
    " ### Crop Images ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b18019e",
   "metadata": {
    "id": "7b18019e"
   },
   "source": [
    "Manually crop dataset images to 1:1 with clear and complete object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b696fdc1",
   "metadata": {
    "id": "b696fdc1"
   },
   "source": [
    "### Create Different Dataset with Corresponding Label ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce972947",
   "metadata": {
    "id": "ce972947"
   },
   "source": [
    "#### 64x64 RGB Dataset ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc253088",
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "aborted",
     "timestamp": 1637237419228,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "fc253088"
   },
   "outputs": [],
   "source": [
    "# resize rgb data to 64x64\n",
    "resize_norm = img_trans = transforms.Compose([transforms.Resize([64,64]),\n",
    "                                              transforms.ToTensor()])\n",
    "    \n",
    "# train data\n",
    "train_data = torch.Tensor([])\n",
    "train_label = []\n",
    "for img in os.listdir(train_img_path):\n",
    "    pic_data = []\n",
    "    breed = img.split('_')[0]\n",
    "    pic_label = dog_dict[breed]\n",
    "    train_label.append(pic_label)\n",
    "    pic = Image.open(os.path.join(train_img_path,img)) \n",
    "    trans_pic = resize_norm(pic)\n",
    "    trans_pic = torch.unsqueeze(trans_pic, dim=0)\n",
    "    train_data = torch.cat((train_data, trans_pic),0)\n",
    "    \n",
    "# test data\n",
    "test_data = torch.Tensor([])\n",
    "test_label = []\n",
    "for img in os.listdir(test_img_path):\n",
    "    pic_data = []\n",
    "    breed = img.split('_')[0]\n",
    "    pic_label = dog_dict[breed]\n",
    "    test_label.append(pic_label)\n",
    "    pic = Image.open(os.path.join(test_img_path,img)) \n",
    "    trans_pic = resize_norm(pic)\n",
    "    trans_pic = torch.unsqueeze(trans_pic, dim=0)\n",
    "    test_data = torch.cat((test_data, trans_pic),0)\n",
    "\n",
    "# change label from list to tensor\n",
    "train_label = torch.tensor(train_label)\n",
    "test_label = torch.tensor(test_label)\n",
    "\n",
    "# save train data and label to .pt file\n",
    "torch.save(train_data, '../dog_dataset/train_data_final.pt')\n",
    "torch.save(train_label, '../dog_dataset/train_label_final.pt')\n",
    "torch.save(test_data, '../dog_dataset/test_data_final.pt')\n",
    "torch.save(test_label, '../dog_dataset/test_label_final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff874d8",
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "aborted",
     "timestamp": 1637237419228,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "dff874d8"
   },
   "outputs": [],
   "source": [
    "utils.show(train_data[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9a687b",
   "metadata": {
    "id": "2a9a687b"
   },
   "source": [
    "#### 64x64 Grayscale Dataset ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34de63a",
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "aborted",
     "timestamp": 1637237419229,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "b34de63a"
   },
   "outputs": [],
   "source": [
    "# rgb to gray\n",
    "rgb_to_gray = transforms.Compose([transforms.Resize([64,64]),\n",
    "                                  transforms.Grayscale(num_output_channels=1),\n",
    "                                  transforms.ToTensor()])\n",
    "\n",
    "# train data\n",
    "train_data_gray = torch.Tensor([])\n",
    "train_label_gray = []\n",
    "for img in os.listdir(train_img_path):\n",
    "    pic_data = []\n",
    "    breed = img.split('_')[0]\n",
    "    pic_label = dog_dict[breed]\n",
    "    train_label_gray.append(pic_label)\n",
    "    pic = Image.open(os.path.join(train_img_path,img))\n",
    "    pic = rgb_to_gray(pic)\n",
    "    train_data_gray = torch.cat((train_data_gray, pic),0)\n",
    "    \n",
    "# test data\n",
    "test_data_gray = torch.Tensor([])\n",
    "test_label_gray = []\n",
    "for img in os.listdir(test_img_path):\n",
    "    pic_data = []\n",
    "    breed = img.split('_')[0]\n",
    "    pic_label = dog_dict[breed]\n",
    "    test_label_gray.append(pic_label)\n",
    "    pic = Image.open(os.path.join(test_img_path,img))\n",
    "    pic = rgb_to_gray(pic)\n",
    "    test_data_gray = torch.cat((test_data_gray, pic),0)\n",
    "    \n",
    "# change label from list to tensor\n",
    "train_label_gray = torch.tensor(train_label_gray)\n",
    "test_label_gray = torch.tensor(test_label_gray)\n",
    "    \n",
    "# save train data and label to .pt file\n",
    "torch.save(train_data_gray, '../dog_dataset/train_data_gray.pt')\n",
    "torch.save(train_label_gray, '../dog_dataset/train_label_gray.pt')\n",
    "torch.save(test_data_gray, '../dog_dataset/test_data_gray.pt')\n",
    "torch.save(test_label_gray, '../dog_dataset/test_label_gray.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bce1ba5",
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "aborted",
     "timestamp": 1637237419229,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "9bce1ba5"
   },
   "outputs": [],
   "source": [
    "utils.show(train_data_gray[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b73143a",
   "metadata": {
    "id": "2b73143a"
   },
   "source": [
    "#### 64x64 Image Augmentation ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc5cbc9",
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "aborted",
     "timestamp": 1637237419229,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "1cc5cbc9"
   },
   "outputs": [],
   "source": [
    "# image augmentation\n",
    "img_augmentation = transforms.Compose([transforms.Resize([64,64]),\n",
    "                                        transforms.RandomHorizontalFlip(),\n",
    "                                        transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "                                        transforms.ToTensor()])\n",
    "\n",
    "#train data\n",
    "train_data_aug = torch.Tensor([])\n",
    "train_label_aug = []\n",
    "for img in os.listdir(train_img_path):\n",
    "    pic_data = []\n",
    "    breed = img.split('_')[0]\n",
    "    pic_label = dog_dict[breed]\n",
    "    train_label_aug.append(pic_label)\n",
    "    pic = Image.open(os.path.join(train_img_path,img)) \n",
    "    trans_pic = img_augmentation(pic)\n",
    "    trans_pic = torch.unsqueeze(trans_pic, dim=0)\n",
    "    train_data_aug = torch.cat((train_data_aug, trans_pic),0)\n",
    "    \n",
    "# change label from list to tensor\n",
    "train_label_aug = torch.tensor(train_label_aug)\n",
    "\n",
    "# save test data and label to .pt file\n",
    "torch.save(train_data_aug, '../dog_dataset/train_data_aug.pt')\n",
    "torch.save(train_label_aug, '../dog_dataset/train_label_aug.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded820b8",
   "metadata": {
    "id": "ded820b8"
   },
   "source": [
    "Create augmented dataset by random flip, brightness and contrast adjustment.  \n",
    "(top: original data, bottom: augmented data)  \n",
    "<div>\n",
    "    <img src=\"report_attachment/image_aug_compare.png\" width='250' align='left'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60c73f7",
   "metadata": {
    "id": "e60c73f7"
   },
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0d98bf66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1637239452516,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "0d98bf66",
    "outputId": "3af26ba4-e239-41a3-c4fe-444c95433a35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device= torch.device(\"cuda\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d6c634",
   "metadata": {
    "id": "f4d6c634"
   },
   "source": [
    "### Load RGD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5bde7f94",
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "aborted",
     "timestamp": 1637237419230,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "5bde7f94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([979, 3, 64, 64])\n",
      "torch.Size([100, 3, 64, 64])\n",
      "torch.Size([979])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "dataset_folder=''\n",
    "train_data=torch.load(dataset_folder+'train_data_final.pt')\n",
    "print(train_data.size())\n",
    "\n",
    "test_data=torch.load(dataset_folder+'test_data_final.pt')\n",
    "print(test_data.size())\n",
    "\n",
    "train_label=torch.load(dataset_folder+'train_label_final.pt')\n",
    "print(train_label.size())\n",
    "\n",
    "test_label=torch.load(dataset_folder+'test_label_final.pt')\n",
    "print(test_label.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a51d839f",
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "aborted",
     "timestamp": 1637237419231,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "a51d839f"
   },
   "outputs": [],
   "source": [
    "class three_layer_net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3,output_size):\n",
    "        super(three_layer_net , self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(  input_size   , hidden_size1  , bias=False  )\n",
    "        self.layer2 = nn.Linear(  hidden_size1 , hidden_size2  , bias=False  )\n",
    "        self.layer3 = nn.Linear(  hidden_size2 , hidden_size3  , bias=False  )\n",
    "        self.layer4 = nn.Linear(  hidden_size3 , output_size   , bias=False  )        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        y       = self.layer1(x)\n",
    "        y_hat   = torch.relu(y)\n",
    "        z       = self.layer2(y_hat)\n",
    "        z_hat   = torch.relu(z)\n",
    "        a       = self.layer3(z_hat)\n",
    "        a_hat   = torch.relu(a)\n",
    "        scores  = self.layer4(a_hat)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "570b9891",
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "aborted",
     "timestamp": 1637237419231,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "570b9891"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three_layer_net(\n",
      "  (layer1): Linear(in_features=12288, out_features=500, bias=False)\n",
      "  (layer2): Linear(in_features=500, out_features=500, bias=False)\n",
      "  (layer3): Linear(in_features=500, out_features=500, bias=False)\n",
      "  (layer4): Linear(in_features=500, out_features=11, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net=three_layer_net(12288,500,500,500,11)\n",
    "print(net)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD( net.parameters() , lr=0.01 )\n",
    "bs= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "236d2782",
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "aborted",
     "timestamp": 1637237419232,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "236d2782"
   },
   "outputs": [],
   "source": [
    "def eval_on_test_set_mlp():\n",
    "\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "\n",
    "\n",
    "    for i in range(0,100,bs):\n",
    "\n",
    "        # extract the minibatch\n",
    "        minibatch_data =  test_data[i:i+bs]\n",
    "        minibatch_label= test_label[i:i+bs]\n",
    "        # send them to the gpu\n",
    "        #minibatch_data=minibatch_data.to(device)\n",
    "        #minibatch_label=minibatch_label.to(device)\n",
    "\n",
    "        # reshape the minibatch\n",
    "        inputs = minibatch_data.view(bs,12288)\n",
    "\n",
    "        # feed it to the network\n",
    "        scores=net( inputs ) \n",
    "\n",
    "        # compute the error made on this batch\n",
    "        error = utils.get_error( scores , minibatch_label)\n",
    "\n",
    "        # add it to the running error\n",
    "        running_error += error.item()\n",
    "\n",
    "        num_batches+=1\n",
    "\n",
    "\n",
    "    # compute error rate on the full test set\n",
    "    total_error = running_error/num_batches\n",
    "\n",
    "    print( 'error rate on test set =', total_error*100 ,'percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d8e8eb7c",
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "aborted",
     "timestamp": 1637237419232,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "d8e8eb7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0 \t time= 0.3800172805786133 \t loss= 2.361804202659843 \t error= 88.24742196761456 percent\n",
      "error rate on test set = 82.9999989271164 percent\n",
      " \n",
      "epoch= 1 \t time= 0.7490966320037842 \t loss= 2.267654445982471 \t error= 83.40206140095425 percent\n",
      "error rate on test set = 82.9999989271164 percent\n",
      " \n",
      "epoch= 2 \t time= 1.12552809715271 \t loss= 2.167452184195371 \t error= 81.64948403220815 percent\n",
      "error rate on test set = 83.00000011920929 percent\n",
      " \n",
      "epoch= 3 \t time= 1.5212228298187256 \t loss= 2.087164253303685 \t error= 77.93814429302806 percent\n",
      "error rate on test set = 78.00000011920929 percent\n",
      " \n",
      "epoch= 4 \t time= 1.9047646522521973 \t loss= 2.0532364193926154 \t error= 78.55670071139778 percent\n",
      "error rate on test set = 67.99999952316284 percent\n",
      " \n",
      "epoch= 5 \t time= 2.3160576820373535 \t loss= 2.024352208855226 \t error= 75.15463884343806 percent\n",
      "error rate on test set = 72.99999952316284 percent\n",
      " \n",
      "epoch= 6 \t time= 2.7075181007385254 \t loss= 1.9858932716330302 \t error= 72.8865979873028 percent\n",
      "error rate on test set = 73.00000011920929 percent\n",
      " \n",
      "epoch= 7 \t time= 3.1622202396392822 \t loss= 1.9406585398408556 \t error= 69.48453611934308 percent\n",
      "error rate on test set = 74.00000035762787 percent\n",
      " \n",
      "epoch= 8 \t time= 3.569734811782837 \t loss= 1.930538691196245 \t error= 72.57731962449772 percent\n",
      "error rate on test set = 80.0 percent\n",
      " \n",
      "epoch= 9 \t time= 3.973123073577881 \t loss= 1.883203084935847 \t error= 68.55670133816827 percent\n",
      "error rate on test set = 71.00000083446503 percent\n",
      " \n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    running_loss=0\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "    \n",
    "    shuffled_indices=torch.randperm(970)\n",
    " \n",
    "    for count in range(0,970,bs):\n",
    "    \n",
    "        # Set the gradients to zeros\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # create a minibatch       \n",
    "        indices=shuffled_indices[count:count+bs]\n",
    "        minibatch_data =  train_data[indices]\n",
    "        minibatch_label=  train_label[indices]\n",
    "        \n",
    "        # send them to the gpu\n",
    "        #minibatch_data=minibatch_data.to(device)\n",
    "        #minibatch_label=minibatch_label.to(device)\n",
    "        \n",
    "        # reshape the minibatch\n",
    "        inputs = minibatch_data.view(bs,12288)\n",
    "\n",
    "        # tell Pytorch to start tracking all operations that will be done on \"inputs\"\n",
    "        inputs.requires_grad_()\n",
    "\n",
    "        # forward the minibatch through the net \n",
    "        scores=net( inputs ) \n",
    "\n",
    "        # Compute the average of the losses of the data points in the minibatch\n",
    "        loss =  criterion( scores , minibatch_label) \n",
    "        \n",
    "        # backward pass to compute dL/dU, dL/dV and dL/dW   \n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent: U=U-lr(dL/dU), V=V-lr(dL/dU), ...\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        # START COMPUTING STATS\n",
    "        \n",
    "        # add the loss of this batch to the running loss\n",
    "        running_loss += loss.detach().item()\n",
    "        \n",
    "        # compute the error made on this batch and add it to the running error       \n",
    "        error = utils.get_error( scores.detach() , minibatch_label)\n",
    "        running_error += error.item()\n",
    "        \n",
    "        num_batches+=1        \n",
    "    \n",
    "    \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss/num_batches\n",
    "    total_error = running_error/num_batches\n",
    "    elapsed = time.time()-start\n",
    "    \n",
    "#if epoch%2 == 0:\n",
    "    print('epoch=',epoch, '\\t time=', elapsed, '\\t loss=', total_loss , '\\t error=', total_error*100 ,'percent')\n",
    "    eval_on_test_set_mlp() \n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b2a915",
   "metadata": {
    "id": "83b2a915"
   },
   "source": [
    "## CNN for Dog Breed Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e7983848",
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1637237419233,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "e7983848"
   },
   "outputs": [],
   "source": [
    "class Vgg_convnet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Vgg_convnet, self).__init__()\n",
    "\n",
    "        # block 1:         3 x 64 x 64 --> 64 x 32 x 32        \n",
    "        self.conv1a = nn.Conv2d(3,   64,  kernel_size=3, padding=1 )\n",
    "        # self.conv1b = nn.Conv2d(64,  64,  kernel_size=3, padding=1 )\n",
    "        self.pool1  = nn.MaxPool2d(2,2)\n",
    "\n",
    "        # block 2:         64 x 32 x 32 --> 128 x 16 x 16\n",
    "        self.conv2a = nn.Conv2d(64,  128, kernel_size=3, padding=1 )\n",
    "        # self.conv2b = nn.Conv2d(128, 128, kernel_size=3, padding=1 )\n",
    "        self.pool2  = nn.MaxPool2d(2,2)\n",
    "\n",
    "        # block 3:         128 x 16 x 16 --> 256 x 8 x 8        \n",
    "        self.conv3a = nn.Conv2d(128, 256, kernel_size=3, padding=1 )\n",
    "        # self.conv3b = nn.Conv2d(256, 256, kernel_size=3, padding=1 )\n",
    "        self.pool3  = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        #block 4:          256 x 8 x 8 --> 512 x 4 x 4\n",
    "        self.conv4a = nn.Conv2d(256, 512, kernel_size=3, padding=1 )\n",
    "        self.pool4  = nn.MaxPool2d(2,2)\n",
    "\n",
    "        #block 5:          512 x 4 x 4 --> 512 x 2 x 2\n",
    "        self.conv5a = nn.Conv2d(512, 512, kernel_size=3, padding=1 )\n",
    "        self.pool5  = nn.MaxPool2d(2,2)\n",
    "\n",
    "        # linear layers:   512 x 2 x 2 --> 32768 --> 4096 --> 4096 --> 10\n",
    "        self.linear1 = nn.Linear(2048, 4096)\n",
    "        self.linear2 = nn.Linear(4096,4096)\n",
    "        self.linear3 = nn.Linear(4096, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # block 1:         3 x 32 x 32 --> 64 x 16 x 16\n",
    "        x = self.conv1a(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # block 2:         64 x 16 x 16 --> 128 x 8 x 8\n",
    "        x = self.conv2a(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # block 3:         128 x 8 x 8 --> 256 x 4 x 4\n",
    "        x = self.conv3a(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        #block 4:          256 x 4 x 4 --> 512 x 2 x 2\n",
    "        x = self.conv4a(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        #block 5:          256 x 4 x 4 --> 256 x 2 x 2\n",
    "        x = self.conv5a(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool5(x)\n",
    "\n",
    "        # linear layers:   512 x 2 x 2 --> 2048 --> 4096 --> 4096 --> 10\n",
    "        x = x.view(-1, 2048)\n",
    "        x = self.linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.linear3(x) \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e798d1f0",
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1637237419233,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "e798d1f0"
   },
   "outputs": [],
   "source": [
    "class LeNet5_convnet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(LeNet5_convnet, self).__init__()\n",
    "\n",
    "        # CL1:   3 x 64 x 64  -->    50 x 64 x 64 \n",
    "        self.conv1 = nn.Conv2d(3,   50,  kernel_size=3,  padding=1 )\n",
    "        \n",
    "        # MP1: 50 x 64 x 64 -->    50 x 32 x 32\n",
    "        self.pool1  = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        # CL2:   50 x 32 x 32  -->    100 x 32 x 32 \n",
    "        self.conv2 = nn.Conv2d(50,  100,  kernel_size=3,  padding=1 )\n",
    "        \n",
    "        # MP2: 100 x 32 x 32 -->    100 x 16 x 16\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        # LL1:   100 x 7 x 7 = 4900 -->  100 \n",
    "        self.linear1 = nn.Linear(25600, 100)\n",
    "        \n",
    "        # LL2:   100  -->  10 \n",
    "        self.linear2 = nn.Linear(100,10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # CL1:   28 x 28  -->    50 x 28 x 28 \n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # MP1: 50 x 28 x 28 -->    50 x 14 x 14\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # CL2:   50 x 14 x 14  -->    100 x 14 x 14\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # MP2: 100 x 14 x 14 -->    100 x 7 x 7\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # LL1:   100 x 7 x 7 = 4900  -->  100 \n",
    "        x = x.view(-1, 25600)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # LL2:   4900  -->  10 \n",
    "        x = self.linear2(x)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "acd55012",
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1637237419234,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "acd55012"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4272)\n",
      "tensor(0.2518)\n"
     ]
    }
   ],
   "source": [
    "mean= train_data.mean()\n",
    "print(mean)\n",
    "std= train_data.std()\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9d9e02ca",
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1637237419234,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "9d9e02ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vgg_convnet(\n",
      "  (conv1a): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2a): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3a): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4a): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv5a): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (linear1): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "  (linear2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (linear3): Linear(in_features=4096, out_features=10, bias=True)\n",
      ")\n",
      "There are 29125770 (29.13 million) parameters in this neural network\n"
     ]
    }
   ],
   "source": [
    "net=Vgg_convnet() #LeNet5_convnet()\n",
    "\n",
    "print(net)\n",
    "utils.display_num_param(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5aba36a2",
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1637237419234,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "5aba36a2"
   },
   "outputs": [],
   "source": [
    "net = net.to(device)\n",
    "mean = mean.to(device)\n",
    "std = std.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f407aebc",
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1637237419235,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "f407aebc"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "my_lr=0.01 \n",
    "bs= 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "78e8fabe",
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1637237419235,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "78e8fabe"
   },
   "outputs": [],
   "source": [
    "def eval_on_test_set_cnn():\n",
    "\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "\n",
    "    for i in range(0,100,bs):\n",
    "\n",
    "        minibatch_data =  test_data[i:i+bs]\n",
    "        minibatch_label= test_label[i:i+bs]\n",
    "\n",
    "        minibatch_data=minibatch_data.to(device)\n",
    "        minibatch_label=minibatch_label.to(device)\n",
    "        \n",
    "        inputs = (minibatch_data - mean)/std\n",
    "        scores=net( inputs ) \n",
    "        error = utils.get_error( scores , minibatch_label)\n",
    "        running_error += error.item()\n",
    "\n",
    "        num_batches+=1\n",
    "\n",
    "    total_error = running_error/num_batches\n",
    "    print( 'error rate on test set =', total_error*100 ,'percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bfff9fb0",
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1637237419235,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "bfff9fb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 10 \t time= 0.05286810000737508 min \t lr= 0.01 \t loss= 2.2955775376289123 \t error= 88.20564516129032 percent\n",
      "error rate on test set = 91.07142857142857 percent\n",
      "-----------------------------\n",
      "epoch= 20 \t time= 0.11283176342646281 min \t lr= 0.01 \t loss= 2.2146960458447857 \t error= 80.51075262408102 percent\n",
      "error rate on test set = 85.71428571428571 percent\n",
      "-----------------------------\n",
      "epoch= 30 \t time= 0.16696158250172932 min \t lr= 0.01 \t loss= 1.9101275397885231 \t error= 68.81720423698425 percent\n",
      "error rate on test set = 79.46428571428571 percent\n",
      "-----------------------------\n",
      "epoch= 40 \t time= 0.22126124699910482 min \t lr= 0.01 \t loss= 1.5959500189750426 \t error= 57.32526875311329 percent\n",
      "error rate on test set = 68.75 percent\n",
      "-----------------------------\n",
      "epoch= 50 \t time= 0.27644235690434776 min \t lr= 0.005 \t loss= 0.7830720553475041 \t error= 25.60483870967742 percent\n",
      "error rate on test set = 60.71428571428571 percent\n",
      "-----------------------------\n",
      "epoch= 60 \t time= 0.3370989203453064 min \t lr= 0.005 \t loss= 0.17911507393563947 \t error= 4.737903225806452 percent\n",
      "error rate on test set = 59.82142857142857 percent\n",
      "-----------------------------\n",
      "epoch= 70 \t time= 0.3922573288281759 min \t lr= 0.005 \t loss= 0.00616454545618786 \t error= 0.0 percent\n",
      "error rate on test set = 58.03571428571429 percent\n",
      "-----------------------------\n",
      "epoch= 80 \t time= 0.44481128056844077 min \t lr= 0.005 \t loss= 0.0025884796078542186 \t error= 0.0 percent\n",
      "error rate on test set = 58.03571428571429 percent\n",
      "-----------------------------\n",
      "epoch= 90 \t time= 0.5005959471066793 min \t lr= 0.005 \t loss= 0.0015835724043427035 \t error= 0.0 percent\n",
      "error rate on test set = 58.03571428571429 percent\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "\n",
    "for epoch in range(1,100):\n",
    "    \n",
    "    # divide the learning rate by 2 at epoch 10, 14 and 18\n",
    "    if epoch == 50: \n",
    "        my_lr = my_lr/2\n",
    "    \n",
    "    # create a new optimizer at the beginning of each epoch: give the current learning rate.   \n",
    "    optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )\n",
    "        \n",
    "    # set the running quatities to zero at the beginning of the epoch\n",
    "    running_loss=0\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "    \n",
    "    # set the order in which to visit the image from the training set\n",
    "    shuffled_indices=torch.randperm(979)\n",
    " \n",
    "    for count in range(0,979,bs):\n",
    "    \n",
    "        # Set the gradients to zeros\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # create a minibatch       \n",
    "        indices=shuffled_indices[count:count+bs]\n",
    "        minibatch_data =  train_data[indices]\n",
    "        minibatch_label=  train_label[indices]\n",
    "        \n",
    "        # send them to the gpu\n",
    "        minibatch_data=minibatch_data.to(device)\n",
    "        minibatch_label=minibatch_label.to(device)\n",
    "        \n",
    "        # normalize the minibatch (this is the only difference compared to before!)\n",
    "        inputs = (minibatch_data - mean)/std\n",
    "        # inputs = minibatch_data\n",
    "        \n",
    "        # tell Pytorch to start tracking all operations that will be done on \"inputs\"\n",
    "        inputs.requires_grad_()\n",
    "\n",
    "        # forward the minibatch through the net \n",
    "        # scores=net( inputs ) \n",
    "        scores=net( inputs )\n",
    "\n",
    "        # Compute the average of the losses of the data points in the minibatch\n",
    "        loss =  criterion( scores , minibatch_label) \n",
    "        \n",
    "        # backward pass to compute dL/dU, dL/dV and dL/dW   \n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent: U=U-lr(dL/dU), V=V-lr(dL/dU), ...\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        # START COMPUTING STATS\n",
    "        \n",
    "        # add the loss of this batch to the running loss\n",
    "        running_loss += loss.detach().item()\n",
    "        \n",
    "        # compute the error made on this batch and add it to the running error       \n",
    "        error = utils.get_error( scores.detach() , minibatch_label)\n",
    "        running_error += error.item()\n",
    "        \n",
    "        num_batches+=1        \n",
    "    \n",
    "    \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss/num_batches\n",
    "    total_error = running_error/num_batches\n",
    "    elapsed = (time.time()-start)/60\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        print('epoch=',epoch, '\\t time=', elapsed,'min','\\t lr=', my_lr  ,'\\t loss=', total_loss , '\\t error=', total_error*100 ,'percent')\n",
    "        eval_on_test_set_cnn() \n",
    "        print('-----------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27b4c11",
   "metadata": {
    "id": "f27b4c11"
   },
   "source": [
    "## RNN Dog Breed Classificaiton on Gray Scale Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "511febd3",
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1637237419236,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "511febd3"
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters for RNN\n",
    "seq_length = 64\n",
    "input_size = 64\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "bs = 50\n",
    "learning_rate = 0.01\n",
    "num_epochs=150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c46efc",
   "metadata": {
    "id": "31c46efc"
   },
   "source": [
    "### Load Gray Scale Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6cda164b",
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1637237419236,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "6cda164b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([979, 64, 64])\n",
      "torch.Size([100, 64, 64])\n",
      "torch.Size([979])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "train_data_gray=torch.load(dataset_folder+'train_data_gray.pt')\n",
    "print(train_data_gray.size())\n",
    "test_data_gray=torch.load(dataset_folder+'test_data_gray.pt')\n",
    "print(test_data_gray.size())\n",
    "\n",
    "\n",
    "train_label_gray=torch.load(dataset_folder+'train_label_gray.pt')\n",
    "print(train_label_gray.size())\n",
    "test_label_gray=torch.load(dataset_folder+'test_label_gray.pt')\n",
    "print(test_label_gray.size())\n",
    "train_dataset_gray = [{'data': train_data_gray[i], 'label': train_label_gray[i]} for i in range(len(train_data_gray)) ]\n",
    "test_dataset_gray = [{'data': test_data_gray[i], 'label': test_label_gray[i]} for i in range(len(test_data_gray)) ]\n",
    "\n",
    "# Data loader\n",
    "train_loader_gray = torch.utils.data.DataLoader(dataset=train_dataset_gray,\n",
    "                                           batch_size=bs,\n",
    "                                           shuffle=True)\n",
    "test_loader_gray = torch.utils.data.DataLoader(dataset=test_dataset_gray,\n",
    "                                          batch_size=bs,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "65f36677",
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1637237419237,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "65f36677"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.layer1 = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.layer2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "\n",
    "        out, _ = self.layer1(x, (h0, c0))\n",
    "        \n",
    "        out = self.layer2(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "24ef36a4",
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1637237419237,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "24ef36a4"
   },
   "outputs": [],
   "source": [
    "def eval_on_test_set_rnn(net):\n",
    "    running_loss = 0\n",
    "    num_batches = 0\n",
    "    num_prediction = 0\n",
    "    num_correct_prediction = 0\n",
    "    \n",
    "    net.eval()\n",
    "    for batch_id, batch in enumerate(test_loader_gray):\n",
    "        \n",
    "        batch_data_num = batch[\"data\"].shape[0]\n",
    "        num_prediction += batch_data_num\n",
    "        \n",
    "        minibatch_data =  batch[\"data\"].to(device)\n",
    "        minibatch_label = batch[\"label\"].to(device)\n",
    "                                  \n",
    "        output  = net( minibatch_data)\n",
    "        predicted_label = torch.argmax(output, dim=1)\n",
    "        num_correct_prediction += torch.sum(minibatch_label == predicted_label).item()\n",
    "                \n",
    "        loss = criterion(  output ,  minibatch_label )    \n",
    "\n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1        \n",
    "    \n",
    "    total_loss = running_loss/num_batches \n",
    "    accuracy = num_correct_prediction / num_prediction\n",
    "    print(f'Test: exp(loss) = {math.exp(total_loss):.4f}\\tTest accuracy = {(accuracy*100):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6f5c255a",
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "aborted",
     "timestamp": 1637237419237,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "6f5c255a"
   },
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "net=RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "83c187bf",
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1637237419238,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "83c187bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \ttime = 0.336\tlr = 0.01\n",
      "Train: exp(loss) = 10.0030\tTrain accuracy = 9.7038\n",
      "Test: exp(loss) = 10.0073\tTest accuracy = 10.0000\n",
      "-----------------------------------\n",
      "Epoch 10 \ttime = 3.703\tlr = 0.01\n",
      "Train: exp(loss) = 9.9161\tTrain accuracy = 12.0531\n",
      "Test: exp(loss) = 9.9860\tTest accuracy = 10.0000\n",
      "-----------------------------------\n",
      "Epoch 20 \ttime = 7.030\tlr = 0.01\n",
      "Train: exp(loss) = 9.8394\tTrain accuracy = 13.4831\n",
      "Test: exp(loss) = 9.8968\tTest accuracy = 12.0000\n",
      "-----------------------------------\n",
      "Epoch 30 \ttime = 10.272\tlr = 0.01\n",
      "Train: exp(loss) = 9.6798\tTrain accuracy = 17.0582\n",
      "Test: exp(loss) = 9.6469\tTest accuracy = 15.0000\n",
      "-----------------------------------\n",
      "Epoch 40 \ttime = 13.581\tlr = 0.01\n",
      "Train: exp(loss) = 9.0595\tTrain accuracy = 20.0204\n",
      "Test: exp(loss) = 8.7009\tTest accuracy = 25.0000\n",
      "-----------------------------------\n",
      "Epoch 50 \ttime = 16.914\tlr = 0.01\n",
      "Train: exp(loss) = 8.4129\tTrain accuracy = 21.7569\n",
      "Test: exp(loss) = 8.0875\tTest accuracy = 24.0000\n",
      "-----------------------------------\n",
      "Epoch 60 \ttime = 20.232\tlr = 0.01\n",
      "Train: exp(loss) = 8.0369\tTrain accuracy = 23.4934\n",
      "Test: exp(loss) = 7.7765\tTest accuracy = 24.0000\n",
      "-----------------------------------\n",
      "Epoch 70 \ttime = 23.572\tlr = 0.01\n",
      "Train: exp(loss) = 7.8734\tTrain accuracy = 21.1440\n",
      "Test: exp(loss) = 7.5407\tTest accuracy = 24.0000\n",
      "-----------------------------------\n",
      "Epoch 80 \ttime = 26.890\tlr = 0.01\n",
      "Train: exp(loss) = 7.7687\tTrain accuracy = 21.8590\n",
      "Test: exp(loss) = 7.4746\tTest accuracy = 22.0000\n",
      "-----------------------------------\n",
      "Epoch 90 \ttime = 30.213\tlr = 0.01\n",
      "Train: exp(loss) = 7.7328\tTrain accuracy = 21.9612\n",
      "Test: exp(loss) = 7.4208\tTest accuracy = 24.0000\n",
      "-----------------------------------\n",
      "Epoch 100 \ttime = 33.575\tlr = 0.01\n",
      "Train: exp(loss) = 7.6451\tTrain accuracy = 22.3698\n",
      "Test: exp(loss) = 7.4934\tTest accuracy = 26.0000\n",
      "-----------------------------------\n",
      "Epoch 110 \ttime = 36.881\tlr = 0.01\n",
      "Train: exp(loss) = 7.6019\tTrain accuracy = 21.4505\n",
      "Test: exp(loss) = 7.2685\tTest accuracy = 25.0000\n",
      "-----------------------------------\n",
      "Epoch 120 \ttime = 40.148\tlr = 0.01\n",
      "Train: exp(loss) = 7.5477\tTrain accuracy = 22.4719\n",
      "Test: exp(loss) = 7.2498\tTest accuracy = 25.0000\n",
      "-----------------------------------\n",
      "Epoch 130 \ttime = 43.382\tlr = 0.01\n",
      "Train: exp(loss) = 7.4920\tTrain accuracy = 21.8590\n",
      "Test: exp(loss) = 7.2807\tTest accuracy = 25.0000\n",
      "-----------------------------------\n",
      "Epoch 140 \ttime = 46.711\tlr = 0.01\n",
      "Train: exp(loss) = 7.5075\tTrain accuracy = 21.9612\n",
      "Test: exp(loss) = 7.3258\tTest accuracy = 23.0000\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # divide the learning rate by 3 except after the first epoch\n",
    "#     if epoch >= 2:\n",
    "#         learning_rate = learning_rate / 3\n",
    "    \n",
    "    # create a new optimizer at the beginning of each epoch: give the current learning rate.   \n",
    "    optimizer=torch.optim.SGD( net.parameters() , lr=learning_rate )\n",
    "\n",
    "    # set the running quatities to zero at the beginning of the epoch\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "    net.train()\n",
    "    num_prediction = 0\n",
    "    num_correct_prediction = 0\n",
    "    for batch_id, batch in enumerate(train_loader_gray):\n",
    "        \n",
    "        # Set the gradients to zeros\n",
    "        optimizer.zero_grad()\n",
    "        batch_data_num = batch[\"data\"].shape[0]\n",
    "        num_prediction += batch_data_num\n",
    "        \n",
    "        # create a minibatch\n",
    "        # send them to the gpu\n",
    "        minibatch_data =  batch[\"data\"].to(device)\n",
    "        minibatch_label = batch[\"label\"].to(device)\n",
    "        \n",
    "        # forward the minibatch through the net        \n",
    "        output  = net( minibatch_data)\n",
    "        predicted_label = torch.argmax(output, dim = 1)\n",
    "        num_correct_prediction += torch.sum(minibatch_label == predicted_label).item()\n",
    "        \n",
    "        # Compute the average of the losses of the data points in this huge batch\n",
    "        loss = criterion(  output ,  minibatch_label )\n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
    "        utils.normalize_gradient(net)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update the running loss  \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "    accuracy = num_correct_prediction / num_prediction\n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss / num_batches\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        print(f'Epoch {epoch} \\ttime = {elapsed:.3f}\\tlr = {learning_rate}\\nTrain: exp(loss) = {math.exp(total_loss):.4f}\\tTrain accuracy = {(accuracy*100):.4f}')\n",
    "        eval_on_test_set_rnn(net)\n",
    "        print(f'-----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8Xe3QdWQ-h3O",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 392,
     "status": "ok",
     "timestamp": 1637239461531,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "8Xe3QdWQ-h3O",
    "outputId": "0386e8c1-ada3-44d5-ca56-6035cfc9faff"
   },
   "outputs": [],
   "source": [
    "# # for testing\n",
    "# if 'google.colab' in sys.modules:\n",
    "#     # mount google drive\n",
    "#     from google.colab import drive\n",
    "#     drive.mount('/content/gdrive')\n",
    "#     # file_name = 'dataset_v1.1.zip'\n",
    "# path_to_file = \"/content/gdrive/MyDrive/dog_data\"\n",
    "# train_data=torch.load(path_to_file+'/train_data_final.pt')\n",
    "# train_label=torch.load(path_to_file+'/train_label_final.pt')\n",
    "# test_data=torch.load(path_to_file+'/test_data_final.pt')\n",
    "# test_label=torch.load(path_to_file+'/test_label_final.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NW3_FMI2BL0e",
   "metadata": {
    "id": "NW3_FMI2BL0e"
   },
   "source": [
    "## Transfer Learning using ResNet\n",
    "Since the performance of our own built MLP and CNN models do not meet our expection (30%~50% error rate), we explore other techniques to improve the accuracy. Transfer learning which focuses on storing knowledge gained while solving one problem and applying it to a different but related problem comes to our minds. We have tested pre-trained models from ResNet and the performance is around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "gKOlfRezCfy4",
   "metadata": {
    "executionInfo": {
     "elapsed": 455,
     "status": "ok",
     "timestamp": 1637239470643,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "gKOlfRezCfy4"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "  data = [x['data'] for x in batch]\n",
    "  label = [x['label'] for x in batch]\n",
    "  data = torch.stack(data, dim=0)\n",
    "  label = torch.stack(label)\n",
    "  return (data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "IytASOjS-iDo",
   "metadata": {
    "executionInfo": {
     "elapsed": 309,
     "status": "ok",
     "timestamp": 1637239480179,
     "user": {
      "displayName": "Zhanhua Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQ-bIAORFfte6cL_NaionlxzQVuengW6gXFywqTg=s64",
      "userId": "12560135567688846267"
     },
     "user_tz": -480
    },
    "id": "IytASOjS-iDo"
   },
   "outputs": [],
   "source": [
    "# prepare dataloader\n",
    "train_dataset = [{'data': train_data[i], 'label': train_label[i]} for i in range(len(train_data)) ]\n",
    "test_dataset = [{'data': test_data[i], 'label': test_label[i]} for i in range(len(test_data)) ]\n",
    "# image_datasets = {'train': [train_data, train_label], 'val': [test_data, test_label]}\n",
    "# dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn = collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn = collate_fn)\n",
    "dataloaders = {'train': train_loader, 'val':test_loader}\n",
    "dataset_sizes = {'train': len(train_dataset), 'val':len(test_dataset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cz0YZnbw-iJj",
   "metadata": {
    "id": "cz0YZnbw-iJj"
   },
   "outputs": [],
   "source": [
    "device= torch.device(\"cuda\")\n",
    "model = models.resnet50(pretrained=True).to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False   \n",
    "    \n",
    "# model.fc = nn.Sequential(nn.Linear(2048, 256), nn.ReLU(inplace=True), nn.Linear(256, 10)).to(device)\n",
    "# model.fc = nn.Sequential(nn.Linear(2048, 256), torch.nn.Tanh(), nn.Linear(256, 10)).to(device)\n",
    "model.fc = nn.Sequential(nn.Linear(2048, 256), nn.Dropout(0.1), nn.Linear(256, 10)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "qaQi4yg5Cxs0",
   "metadata": {
    "id": "qaQi4yg5Cxs0"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} Current LR: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc, current_lr))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "BzP6PwoV-iL-",
   "metadata": {
    "id": "BzP6PwoV-iL-"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "86b05c42",
   "metadata": {
    "id": "86b05c42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 2.2975 Acc: 0.1185 Current LR: 0.0010\n",
      "val Loss: 2.2333 Acc: 0.1500 Current LR: 0.0010\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 2.2154 Acc: 0.2033 Current LR: 0.0010\n",
      "val Loss: 2.0626 Acc: 0.3300 Current LR: 0.0010\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 2.1435 Acc: 0.2717 Current LR: 0.0010\n",
      "val Loss: 1.9636 Acc: 0.3900 Current LR: 0.0010\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 2.0642 Acc: 0.3555 Current LR: 0.0010\n",
      "val Loss: 1.8472 Acc: 0.4000 Current LR: 0.0010\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 1.9876 Acc: 0.3739 Current LR: 0.0010\n",
      "val Loss: 1.6486 Acc: 0.5100 Current LR: 0.0010\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 1.9270 Acc: 0.4045 Current LR: 0.0010\n",
      "val Loss: 1.6593 Acc: 0.4700 Current LR: 0.0010\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 1.8674 Acc: 0.3851 Current LR: 0.0001\n",
      "val Loss: 1.5989 Acc: 0.5400 Current LR: 0.0001\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 1.8285 Acc: 0.4341 Current LR: 0.0001\n",
      "val Loss: 1.6062 Acc: 0.4700 Current LR: 0.0001\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 1.8116 Acc: 0.4464 Current LR: 0.0001\n",
      "val Loss: 1.5796 Acc: 0.4900 Current LR: 0.0001\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 1.8173 Acc: 0.4362 Current LR: 0.0001\n",
      "val Loss: 1.4890 Acc: 0.5300 Current LR: 0.0001\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 1.7985 Acc: 0.4668 Current LR: 0.0001\n",
      "val Loss: 1.4373 Acc: 0.5400 Current LR: 0.0001\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 1.8174 Acc: 0.4311 Current LR: 0.0001\n",
      "val Loss: 1.4915 Acc: 0.5300 Current LR: 0.0001\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 1.8114 Acc: 0.4362 Current LR: 0.0001\n",
      "val Loss: 1.4858 Acc: 0.5100 Current LR: 0.0001\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 1.7975 Acc: 0.4607 Current LR: 0.0000\n",
      "val Loss: 1.6093 Acc: 0.5200 Current LR: 0.0000\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 1.8105 Acc: 0.4362 Current LR: 0.0000\n",
      "val Loss: 1.4262 Acc: 0.5100 Current LR: 0.0000\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 1.7866 Acc: 0.4597 Current LR: 0.0000\n",
      "val Loss: 1.5468 Acc: 0.5200 Current LR: 0.0000\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 1.8008 Acc: 0.4423 Current LR: 0.0000\n",
      "val Loss: 1.5475 Acc: 0.5100 Current LR: 0.0000\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 1.7666 Acc: 0.4719 Current LR: 0.0000\n",
      "val Loss: 1.5090 Acc: 0.5600 Current LR: 0.0000\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 1.7840 Acc: 0.4545 Current LR: 0.0000\n",
      "val Loss: 1.5029 Acc: 0.5300 Current LR: 0.0000\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 1.8015 Acc: 0.4556 Current LR: 0.0000\n",
      "val Loss: 1.4918 Acc: 0.5400 Current LR: 0.0000\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 1.7676 Acc: 0.4770 Current LR: 0.0000\n",
      "val Loss: 1.4648 Acc: 0.5500 Current LR: 0.0000\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 1.7972 Acc: 0.4372 Current LR: 0.0000\n",
      "val Loss: 1.5505 Acc: 0.5300 Current LR: 0.0000\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 1.7838 Acc: 0.4535 Current LR: 0.0000\n",
      "val Loss: 1.5218 Acc: 0.5100 Current LR: 0.0000\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 1.7742 Acc: 0.4545 Current LR: 0.0000\n",
      "val Loss: 1.5229 Acc: 0.5000 Current LR: 0.0000\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 1.7571 Acc: 0.4443 Current LR: 0.0000\n",
      "val Loss: 1.5872 Acc: 0.5200 Current LR: 0.0000\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 1.7802 Acc: 0.4545 Current LR: 0.0000\n",
      "val Loss: 1.6269 Acc: 0.4900 Current LR: 0.0000\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 1.7930 Acc: 0.4423 Current LR: 0.0000\n",
      "val Loss: 1.5087 Acc: 0.5300 Current LR: 0.0000\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 1.7618 Acc: 0.4586 Current LR: 0.0000\n",
      "val Loss: 1.5590 Acc: 0.4800 Current LR: 0.0000\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 1.7698 Acc: 0.4729 Current LR: 0.0000\n",
      "val Loss: 1.5542 Acc: 0.5300 Current LR: 0.0000\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 1.7926 Acc: 0.4382 Current LR: 0.0000\n",
      "val Loss: 1.5428 Acc: 0.5200 Current LR: 0.0000\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 1.7921 Acc: 0.4515 Current LR: 0.0000\n",
      "val Loss: 1.5439 Acc: 0.5300 Current LR: 0.0000\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 1.8193 Acc: 0.4188 Current LR: 0.0000\n",
      "val Loss: 1.4825 Acc: 0.5300 Current LR: 0.0000\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 1.7914 Acc: 0.4423 Current LR: 0.0000\n",
      "val Loss: 1.5174 Acc: 0.5000 Current LR: 0.0000\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 1.7859 Acc: 0.4648 Current LR: 0.0000\n",
      "val Loss: 1.5240 Acc: 0.4900 Current LR: 0.0000\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 1.7854 Acc: 0.4515 Current LR: 0.0000\n",
      "val Loss: 1.5866 Acc: 0.4900 Current LR: 0.0000\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 1.7990 Acc: 0.4382 Current LR: 0.0000\n",
      "val Loss: 1.4790 Acc: 0.5100 Current LR: 0.0000\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 1.8102 Acc: 0.4259 Current LR: 0.0000\n",
      "val Loss: 1.5204 Acc: 0.5300 Current LR: 0.0000\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 1.7912 Acc: 0.4566 Current LR: 0.0000\n",
      "val Loss: 1.4581 Acc: 0.5100 Current LR: 0.0000\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 1.7996 Acc: 0.4351 Current LR: 0.0000\n",
      "val Loss: 1.5659 Acc: 0.4700 Current LR: 0.0000\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 1.7933 Acc: 0.4433 Current LR: 0.0000\n",
      "val Loss: 1.4926 Acc: 0.4900 Current LR: 0.0000\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 1.7881 Acc: 0.4566 Current LR: 0.0000\n",
      "val Loss: 1.5061 Acc: 0.5200 Current LR: 0.0000\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 1.7931 Acc: 0.4464 Current LR: 0.0000\n",
      "val Loss: 1.4468 Acc: 0.5400 Current LR: 0.0000\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 1.7804 Acc: 0.4484 Current LR: 0.0000\n",
      "val Loss: 1.5372 Acc: 0.5600 Current LR: 0.0000\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 1.7890 Acc: 0.4433 Current LR: 0.0000\n",
      "val Loss: 1.5135 Acc: 0.5400 Current LR: 0.0000\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 1.7568 Acc: 0.4740 Current LR: 0.0000\n",
      "val Loss: 1.4662 Acc: 0.5400 Current LR: 0.0000\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 1.7855 Acc: 0.4474 Current LR: 0.0000\n",
      "val Loss: 1.5099 Acc: 0.5300 Current LR: 0.0000\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 1.8148 Acc: 0.4208 Current LR: 0.0000\n",
      "val Loss: 1.4849 Acc: 0.5100 Current LR: 0.0000\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 1.7945 Acc: 0.4443 Current LR: 0.0000\n",
      "val Loss: 1.5360 Acc: 0.5000 Current LR: 0.0000\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 1.7891 Acc: 0.4341 Current LR: 0.0000\n",
      "val Loss: 1.5382 Acc: 0.4800 Current LR: 0.0000\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 1.7766 Acc: 0.4413 Current LR: 0.0000\n",
      "val Loss: 1.5916 Acc: 0.5200 Current LR: 0.0000\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 1.7759 Acc: 0.4597 Current LR: 0.0000\n",
      "val Loss: 1.5743 Acc: 0.5000 Current LR: 0.0000\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 1.7794 Acc: 0.4464 Current LR: 0.0000\n",
      "val Loss: 1.4909 Acc: 0.5100 Current LR: 0.0000\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 1.7915 Acc: 0.4515 Current LR: 0.0000\n",
      "val Loss: 1.5135 Acc: 0.5400 Current LR: 0.0000\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 1.7756 Acc: 0.4586 Current LR: 0.0000\n",
      "val Loss: 1.4004 Acc: 0.5700 Current LR: 0.0000\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 1.7935 Acc: 0.4556 Current LR: 0.0000\n",
      "val Loss: 1.4932 Acc: 0.5300 Current LR: 0.0000\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 1.7698 Acc: 0.4586 Current LR: 0.0000\n",
      "val Loss: 1.4934 Acc: 0.5000 Current LR: 0.0000\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 1.7850 Acc: 0.4402 Current LR: 0.0000\n",
      "val Loss: 1.4127 Acc: 0.6000 Current LR: 0.0000\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 1.7759 Acc: 0.4740 Current LR: 0.0000\n",
      "val Loss: 1.4575 Acc: 0.5400 Current LR: 0.0000\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 1.7636 Acc: 0.4658 Current LR: 0.0000\n",
      "val Loss: 1.4819 Acc: 0.5100 Current LR: 0.0000\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 1.7728 Acc: 0.4597 Current LR: 0.0000\n",
      "val Loss: 1.5296 Acc: 0.5300 Current LR: 0.0000\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 1.7811 Acc: 0.4484 Current LR: 0.0000\n",
      "val Loss: 1.5047 Acc: 0.5200 Current LR: 0.0000\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 1.8214 Acc: 0.4290 Current LR: 0.0000\n",
      "val Loss: 1.4154 Acc: 0.5600 Current LR: 0.0000\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 1.7691 Acc: 0.4535 Current LR: 0.0000\n",
      "val Loss: 1.4828 Acc: 0.5100 Current LR: 0.0000\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 1.8007 Acc: 0.4535 Current LR: 0.0000\n",
      "val Loss: 1.5592 Acc: 0.5100 Current LR: 0.0000\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 1.7883 Acc: 0.4515 Current LR: 0.0000\n",
      "val Loss: 1.5316 Acc: 0.4700 Current LR: 0.0000\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 1.7806 Acc: 0.4740 Current LR: 0.0000\n",
      "val Loss: 1.4641 Acc: 0.5100 Current LR: 0.0000\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 1.8133 Acc: 0.4321 Current LR: 0.0000\n",
      "val Loss: 1.5177 Acc: 0.5500 Current LR: 0.0000\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 1.7896 Acc: 0.4311 Current LR: 0.0000\n",
      "val Loss: 1.4402 Acc: 0.5200 Current LR: 0.0000\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.8069 Acc: 0.4362 Current LR: 0.0000\n",
      "val Loss: 1.4645 Acc: 0.5000 Current LR: 0.0000\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 1.7815 Acc: 0.4556 Current LR: 0.0000\n",
      "val Loss: 1.5115 Acc: 0.5500 Current LR: 0.0000\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 1.7935 Acc: 0.4413 Current LR: 0.0000\n",
      "val Loss: 1.5164 Acc: 0.5200 Current LR: 0.0000\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 1.7897 Acc: 0.4719 Current LR: 0.0000\n",
      "val Loss: 1.4770 Acc: 0.5600 Current LR: 0.0000\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 1.7818 Acc: 0.4443 Current LR: 0.0000\n",
      "val Loss: 1.5600 Acc: 0.5000 Current LR: 0.0000\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 1.8043 Acc: 0.4576 Current LR: 0.0000\n",
      "val Loss: 1.5946 Acc: 0.5400 Current LR: 0.0000\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 1.8045 Acc: 0.4331 Current LR: 0.0000\n",
      "val Loss: 1.4852 Acc: 0.5800 Current LR: 0.0000\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 1.7662 Acc: 0.4525 Current LR: 0.0000\n",
      "val Loss: 1.4675 Acc: 0.5400 Current LR: 0.0000\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 1.7880 Acc: 0.4331 Current LR: 0.0000\n",
      "val Loss: 1.5548 Acc: 0.5000 Current LR: 0.0000\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 1.8059 Acc: 0.4607 Current LR: 0.0000\n",
      "val Loss: 1.5155 Acc: 0.5100 Current LR: 0.0000\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 1.7913 Acc: 0.4351 Current LR: 0.0000\n",
      "val Loss: 1.5417 Acc: 0.5000 Current LR: 0.0000\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 1.8243 Acc: 0.4116 Current LR: 0.0000\n",
      "val Loss: 1.5674 Acc: 0.5100 Current LR: 0.0000\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 1.7740 Acc: 0.4443 Current LR: 0.0000\n",
      "val Loss: 1.5246 Acc: 0.5200 Current LR: 0.0000\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 1.7662 Acc: 0.4688 Current LR: 0.0000\n",
      "val Loss: 1.4831 Acc: 0.5300 Current LR: 0.0000\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 1.7893 Acc: 0.4382 Current LR: 0.0000\n",
      "val Loss: 1.4544 Acc: 0.5200 Current LR: 0.0000\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 1.7743 Acc: 0.4688 Current LR: 0.0000\n",
      "val Loss: 1.5198 Acc: 0.5200 Current LR: 0.0000\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 1.7852 Acc: 0.4413 Current LR: 0.0000\n",
      "val Loss: 1.5640 Acc: 0.5200 Current LR: 0.0000\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 1.7904 Acc: 0.4668 Current LR: 0.0000\n",
      "val Loss: 1.5714 Acc: 0.5200 Current LR: 0.0000\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 1.8024 Acc: 0.4311 Current LR: 0.0000\n",
      "val Loss: 1.5101 Acc: 0.5400 Current LR: 0.0000\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 1.7963 Acc: 0.4423 Current LR: 0.0000\n",
      "val Loss: 1.5584 Acc: 0.5300 Current LR: 0.0000\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 1.7855 Acc: 0.4576 Current LR: 0.0000\n",
      "val Loss: 1.4612 Acc: 0.5100 Current LR: 0.0000\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 1.7856 Acc: 0.4454 Current LR: 0.0000\n",
      "val Loss: 1.4215 Acc: 0.5600 Current LR: 0.0000\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 1.7888 Acc: 0.4413 Current LR: 0.0000\n",
      "val Loss: 1.5560 Acc: 0.5400 Current LR: 0.0000\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 1.7626 Acc: 0.4760 Current LR: 0.0000\n",
      "val Loss: 1.4606 Acc: 0.5200 Current LR: 0.0000\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 1.7838 Acc: 0.4545 Current LR: 0.0000\n",
      "val Loss: 1.4835 Acc: 0.5300 Current LR: 0.0000\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 1.8009 Acc: 0.4239 Current LR: 0.0000\n",
      "val Loss: 1.5477 Acc: 0.4700 Current LR: 0.0000\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 1.7979 Acc: 0.4311 Current LR: 0.0000\n",
      "val Loss: 1.4342 Acc: 0.5400 Current LR: 0.0000\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 1.8110 Acc: 0.4423 Current LR: 0.0000\n",
      "val Loss: 1.4980 Acc: 0.4900 Current LR: 0.0000\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 1.7950 Acc: 0.4576 Current LR: 0.0000\n",
      "val Loss: 1.4446 Acc: 0.5700 Current LR: 0.0000\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 1.7811 Acc: 0.4637 Current LR: 0.0000\n",
      "val Loss: 1.5132 Acc: 0.5200 Current LR: 0.0000\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 1.8001 Acc: 0.4413 Current LR: 0.0000\n",
      "val Loss: 1.5968 Acc: 0.5100 Current LR: 0.0000\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 1.7831 Acc: 0.4617 Current LR: 0.0000\n",
      "val Loss: 1.4724 Acc: 0.5500 Current LR: 0.0000\n",
      "\n",
      "Training complete in 3m 11s\n",
      "Best val Acc: 0.600000\n"
     ]
    }
   ],
   "source": [
    "model_trained = train_model(model, criterion, optimizer, scheduler, num_epochs=100)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ce972947",
    "2a9a687b",
    "2b73143a"
   ],
   "name": "data_MLP_CNN_RNN_TR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
